{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation with Amazon Bedrock - Retrieving Data Automatically from APIs\n",
    "\n",
    "> *PLEASE NOTE: This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "---\n",
    "\n",
    "Throughout this workshop so far, we have been working with unstructured text retrieval via semantic similarity search. However, another important type of retrieval which customers can take advantage of with Amazon Bedrock is **structured data retrieval** from APIs. Structured data retrieval is extremely useful for augmenting LLM applications with up to date information which can be retrieved in a repeatable manner, but the outputs are always changing. An example of a question you might ask an LLM which uses this type of retrieval might be \"How long will it take for my Amazon.com order containing socks to arrive?\". In this notebook, we will show how to integrate an LLM with a backend API service which has the ability to answer a user's question through RAG.\n",
    "\n",
    "Specifically, we will be building a tool which is able to tell you the weather based on natural language. This is a fairly trivial example, but it does a good job of showing how multiple API tools can be used by an LLM to retrieve dynamic data to augment a prompt. Here is a visual of the architecture we will be building today.\n",
    "\n",
    "![api](./images/api.png)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7cced6",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d7993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.environment_validation import validate_environment, validate_model_access\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ac361",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_models = [\n",
    "    \"amazon.titan-embed-text-v1\",\n",
    "    \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "]\n",
    "validate_model_access(required_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85063bca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "import xmltodict\n",
    "from rich import print as rprint\n",
    "\n",
    "from utils import bedrock\n",
    "from utils.prompt_utils import extract_docstring_info, construct_format_tool_for_claude_prompt, prompts_to_messages_converse\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f3e58",
   "metadata": {},
   "source": [
    "---\n",
    "## Defining the API Tools\n",
    "\n",
    "The first thing we need to do for our LLM is define the tools it has access to. In this case we will be defining local Python functions, but it is important to note that these could be any type of application service. Examples of what these tools might be on AWS include...\n",
    "\n",
    "* An AWS Lambda function\n",
    "* An Amazon RDS database connection\n",
    "* An Amazon DynamnoDB table\n",
    "  \n",
    "More generic examples include...\n",
    "\n",
    "* REST APIs\n",
    "* Data warehouses, data lakes, and databases\n",
    "* Computation engines\n",
    "\n",
    "In this case, we define two tools which reach external APIs below with two python functions\n",
    "1. the ability to retrieve the latitude and longitude of a place given a natural language input\n",
    "2. the ability to retrieve the weather given an input latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb0dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_weather(latitude: str, longitude: str):\n",
    "    \"\"\"\n",
    "    Fetches and returns the current weather for a given latitude and longitude from the Open-Meteo API.\n",
    "\n",
    "    Args:\n",
    "        latitude (str): The latitude of the location to fetch the weather for.\n",
    "        longitude (str): The longitude of the location to fetch the weather for.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the current weather data for the given location.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "def get_lat_long(place: str):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fetches and returns the latitude and longitude of a given place from the Nominatim OpenStreetMap API.\n",
    "\n",
    "    Args:\n",
    "        place (str): The name of the place to fetch the latitude and longitude for.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the latitude and longitude of the given place, or None if the place was not found.\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "    params = {'q': place, 'format': 'json', 'limit': 1}\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, params=params, headers=headers).json()\n",
    "    if response:\n",
    "        lat = response[0][\"lat\"]\n",
    "        lon = response[0][\"lon\"]\n",
    "        return {\"latitude\": lat, \"longitude\": lon}\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f2f2b",
   "metadata": {},
   "source": [
    "The [Converse](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html) api within Amazon Bedrock supports native tool usage for an array of Models from Anthropic, Mistral, and Cohere. We will use this api to have the model dynamically fetch the weather based on the user's input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afe2ca1",
   "metadata": {},
   "source": [
    "To help make the solution a bit more robust, we will create pydantic models for each function input. This will help us validate the input data that will be generated by the model and will also produce an json input schema that can be used to construct the tool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76cd0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc2b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetLocationInput(BaseModel):\n",
    "    place: str = Field(..., description=\"The name of the place to fetch the latitude and longitude for.\")\n",
    "\n",
    "class GetWeatherInput(BaseModel):\n",
    "    latitude: str = Field(..., description=\"The latitude of the location to fetch the weather for.\")\n",
    "    longitude: str = Field(..., description=\"The longitude of the location to fetch the weather for.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a23f69",
   "metadata": {},
   "source": [
    "The code below will create the tool specification for the two tools we will be using in this example. For more details see [here](https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c76513",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = []\n",
    "tool_lookup = {}\n",
    "for func, inp_obj in zip(\n",
    "    [get_lat_long, get_weather], [GetLocationInput, GetWeatherInput]\n",
    "):\n",
    "    tool_lookup[func.__name__] = {\"func\": func, \"input\": inp_obj}\n",
    "    tools.append(\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": func.__name__,\n",
    "                \"description\": func.__doc__,\n",
    "                \"inputSchema\": {\"json\": inp_obj.model_json_schema()}, # much easier than having to write the schema manually especially for more complex functions\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "tool_config = {\"tools\": tools}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18203223",
   "metadata": {},
   "source": [
    "---\n",
    "## Executing the RAG Workflow\n",
    "\n",
    "Armed with our prompt and structured tools, we can now write an orchestration function which will iteratively step through the logical tasks to answer a user question. In the cell below we use the `invoke_model` function to generate a response with Claude and the `single_retriever_step` function to iteratively call tools when the LLM tells us we need to. The general flow works like this...\n",
    "\n",
    "1. The user enters an input to the application\n",
    "2. The user input is merged with the original prompt and sent to the LLM to determine the next step\n",
    "3. If the LLM knows the answer, it will answer and we are done. If not, go to next step 4.\n",
    "4. The LLM will determine which tool to use to answer the question.\n",
    "5. We will use the tool as directed by the LLM and retrieve the results.\n",
    "6. We provide the results back into the original prompt as more context.\n",
    "7. We ask the LLM the next step or if knows the answer.\n",
    "8. Return to step 3.\n",
    "\n",
    "We can orchestrate the entire workflow through a simple while loop where we continuously invoke the LLM until it has enough information to provide the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_modelId = \"anthropic.claude-3-haiku-20240307-v1:0\" # model ID for the LLM model to use\n",
    "\n",
    "# user's question\n",
    "prompt = \"What is the weather in New York?\"\n",
    "messages = prompts_to_messages_converse([{\"role\": \"user\", \"text_prompt\": prompt}])\n",
    "\n",
    "# system message to define the role and behavior of the assistant\n",
    "system_message = \"\"\"\n",
    "You are a meterological assistant.\n",
    "Your job is to answer weather-related questions. You have access to tools to look up weather information for a given location.\n",
    "You may need to utilize multiple tools to answer the user's question.\n",
    "Place any intermediate thoughts into <thinking> tags\n",
    "When you have the final answer, provide it in <answer> tags\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "inference_config = {\"temperature\": 0.0}\n",
    "\n",
    "rprint(f\"[bold #219ebc]Initial prompt:[/bold #219ebc] [#ffb703]{prompt}[/#ffb703]\")\n",
    "\n",
    "\n",
    "# The execution workflow will run in a loop until the assistant provides a final answer\n",
    "while True:\n",
    "\n",
    "    # invoke the model with the messages (conversation state), system prompt, and tool configuration\n",
    "    response = boto3_bedrock.converse(\n",
    "        modelId=llm_modelId,\n",
    "        messages=messages,\n",
    "        system=[{\"text\": system_message}],\n",
    "        inferenceConfig=inference_config,\n",
    "        toolConfig=tool_config,\n",
    "        # additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    response_messages = response[\"output\"][\"message\"]\n",
    "    response_content = response_messages[\"content\"]\n",
    "    messages.append(response_messages)\n",
    "\n",
    "    # if the model determines that a tool should be used, it will provide \"tool_use\" as the stop reason\n",
    "    if response[\"stopReason\"] == \"tool_use\":\n",
    "\n",
    "        # model may respond with both the tool_use response and some intermediate thoughts\n",
    "        text_content = next((m['text'] for m in response_content if 'text' in m), \"\")\n",
    "        \n",
    "        tool_use_content = [m for m in response_content if \"toolUse\" in m][0]\n",
    "\n",
    "        # from the tool_use response, extract the tool name, and the input to the tool\n",
    "        tool_use_id = tool_use_content[\"toolUse\"][\"toolUseId\"]\n",
    "        tool_name = tool_use_content[\"toolUse\"][\"name\"]\n",
    "        tool_input = tool_use_content[\"toolUse\"][\"input\"]\n",
    "        \n",
    "        rprint(f\"[bold #219ebc]Intermediate response:[/bold #219ebc] [#ffb703]{text_content}[#ffb703]\")\n",
    "        rprint(f\"[bold #219ebc]Tool name:[/bold #219ebc] [#ffb703]{tool_name}[#ffb703]\")\n",
    "        rprint(f\"[bold #219ebc]Tool input:[/bold #219ebc] [#ffb703]{tool_input}[#ffb703]\")\n",
    "\n",
    "        try:\n",
    "            # get the function to invoke\n",
    "            function = tool_lookup[tool_name][\"func\"]\n",
    "            \n",
    "            # get the pydanitc model to parse the input\n",
    "            function_object = tool_lookup[tool_name][\"input\"]\n",
    "            \n",
    "            # parse the input\n",
    "            tool_input_obj = function_object.parse_obj(tool_input)\n",
    "            \n",
    "            # invoke the function with the parsed input\n",
    "            result = function(**tool_input_obj.dict())\n",
    "            rprint(f\"[bold #219ebc]Tool output:[/bold #219ebc] [#ffb703]{result}[#ffb703]\")\n",
    "            \n",
    "            \n",
    "            # we have to attach the tool response to the messages (conversation history) to continue the conversation\n",
    "            tool_response_message = prompts_to_messages_converse(\n",
    "                [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"tool_use_id\": tool_use_id,\n",
    "                        \"text_prompt\": f\"The output of {tool_name} is {result}\",\n",
    "                        \"tool_status\": \"success\",\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            messages.extend(tool_response_message)\n",
    "\n",
    "        # in case there is an invocation error, we will attach the error message to the messages so that the model can make corrections\n",
    "        except Exception as e:\n",
    "            error = str(e)\n",
    "            tool_response_message = prompts_to_messages_converse(\n",
    "                [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"tool_use_id\": tool_use_id,\n",
    "                        \"text_prompt\": result,\n",
    "                        \"tool_status\": \"error\",\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            messages.extend(tool_response_message)\n",
    "            \n",
    "    # if generation stopped for any other reason\n",
    "    else:\n",
    "        text_content = [m for m in response_content if \"text\" in m][0]\n",
    "        result = text_content[\"text\"]\n",
    "        if \"<answer>\" in result:\n",
    "            break\n",
    "\n",
    "final_answer = result.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "rprint(f\"[bold #219ebc]Final answer:[/bold #219ebc] [#ffb703]{final_answer}[#ffb703]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70b217d",
   "metadata": {},
   "source": [
    "---\n",
    "## Next steps\n",
    "\n",
    "Now that you have used a few different retrieval systems, lets move on to the next notebook where you can apply the skills you've learned so far!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7fcb83",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
