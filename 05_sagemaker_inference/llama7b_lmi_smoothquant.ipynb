{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a729a7fc-a1b6-4832-9155-d240ccd8ecc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy Llama2 7b SmoothQuant Model with high performance on SageMaker using Sagemaker LMI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2958bcf-767f-4cd7-826e-963f91565876",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "In this lab we explore how to host a Llama2 7b large language model with FP16 precision SmoothQuant quantized, on SageMaker using DeepSpeed. We use DJLServing as the model serving solution in this example that is bundled in the LMI container. DJLServing is a high-performance universal model serving solution powered by the Deep Java Library (DJL) that is programming language agnostic. To learn more about DJL and DJLServing, you can refer to our recent blog post (https://aws.amazon.com/blogs/machine-learning/deploy-bloom-176b-and-opt-30b-on-amazon-sagemaker-with-large-model-inference-deep-learning-containers-and-deepspeed/).\n",
    "\n",
    "\n",
    "Model parallelism can help deploy large models that would normally be too large for a single GPU. With model parallelism, we partition and distribute a model across multiple GPUs. Each GPU holds a different part of the model, resolving the memory capacity issue for the largest deep learning models with billions of parameters. \n",
    "\n",
    "SageMaker has rolled out DeepSpeed container which now provides users with the ability to leverage the managed serving capabilities and help to provide the un-differentiated heavy lifting.\n",
    "\n",
    "In this notebook, we deploy https://huggingface.co/TheBloke/Llama-2-7b-fp16 model on a ml.g5.2xlarge instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532764e-76e5-4f2b-8393-0cebd99056c8",
   "metadata": {},
   "source": [
    "# Licence agreement\n",
    " - View license information https://huggingface.co/meta-llama before using the model.\n",
    " - This notebook is a sample notebook and not intended for production use. Please refer to the licence at https://github.com/aws/mit-0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd6a36f-ee95-453d-a127-c8a7de6a026d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq sagemaker\n",
    "%pip install -Uq transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf0c89f4-679c-4557-b95d-1d954c15a020",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "561b79ba-7354-4d40-87ce-dc5813092576",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f68b5181-d018-4564-9762-fa8770a9672f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_bucket = sess.default_bucket()  # bucket to house model artifacts\n",
    "\n",
    "llama2_7b_fp16_smoothquant_s3_code_prefix = \"hf-large-model-djl/meta-llama/Llama2-7b-fp16-smoothquant/code\"  # folder within bucket where code artifact will go\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913de41-193a-4bfb-b42e-581c7677d0ed",
   "metadata": {},
   "source": [
    "## Create SageMaker compatible Model artifact,  upload Model to S3 and bring your own inference script.\n",
    "\n",
    "SageMaker Large Model Inference containers can be used to host models without providing your own inference code. This is extremely useful when there is no custom pre-processing of the input data or postprocessing of the model's predictions.\n",
    "\n",
    "SageMaker needs the model artifacts to be in a Tarball format. In this example, we provide the following files - serving.properties.\n",
    "\n",
    "The tarball is in the following format:\n",
    "\n",
    "```\n",
    "code\n",
    "├──── \n",
    "│   └── serving.properties\n",
    "```\n",
    "\n",
    "    serving.properties is the configuration file that can be used to configure the model server.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f5d44-6388-49fa-bc87-2e82bed226f3",
   "metadata": {},
   "source": [
    "#### Create serving.properties \n",
    "This is a configuration file to indicate to DJL Serving which model parallelization and inference optimization libraries you would like to use. Depending on your need, you can set the appropriate configuration.\n",
    "\n",
    "Here is a list of settings that we use in this configuration file -\n",
    "\n",
    "    engine: The engine for DJL to use. In this case, we have set it to MPI.\n",
    "    option.model_id: The model id of a pretrained model hosted inside a model repository on huggingface.co (https://huggingface.co/models) or S3 path to the model artefacts. \n",
    "    option.tensor_parallel_degree: Set to the number of GPU devices over which Accelerate needs to partition the model. This parameter also controls the no of workers per model which will be started up when DJL serving runs. As an example if we have a 4 GPU machine and we are creating 4 partitions then we will have 1 worker per model to serve the requests.\n",
    "    option.dtype: The precision with which you want to run inference.\n",
    "    option.quantize: quantization method (bitsandbytes, gptq, smoothquant).\n",
    "\n",
    "For more details on the configuration options and an exhaustive list, you can refer the documentation - https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2691f5b-4e98-4f7a-887c-49c05bbf7a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf code_llama2_7b_fp16_smoothquant\n",
    "!mkdir -p code_llama2_7b_fp16_smoothquant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b040a8-2180-46cd-aa5c-b1f6d50c2dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code_llama2_7b_fp16_smoothquant/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile code_llama2_7b_fp16_smoothquant/serving.properties\n",
    "engine=DeepSpeed\n",
    "option.model_id=TheBloke/Llama-2-7B-fp16\n",
    "option.tensor_parallel_degree=1\n",
    "option.dtype=fp16\n",
    "option.quantize=smoothquant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "996ebeb2-cfee-4e7b-af8f-3ccc811fa1eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t\u001b[36mengine\u001b[39;49;00m=\u001b[33mDeepSpeed\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     2\t\u001b[36moption.model_id\u001b[39;49;00m=\u001b[33mTheBloke/Llama-2-7B-fp16\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     3\t\u001b[36moption.tensor_parallel_degree\u001b[39;49;00m=\u001b[33m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     4\t\u001b[36moption.dtype\u001b[39;49;00m=\u001b[33mfp16\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     5\t\u001b[36moption.quantize\u001b[39;49;00m=\u001b[33msmoothquant\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code_llama2_7b_fp16_smoothquant/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9db9c4-5023-4125-a413-7c5afa135218",
   "metadata": {},
   "source": [
    "**Image URI for the DJL container is being used here**\n",
    "\n",
    "We use `image_uris.retrieve` function to generate an ECR image URI for the pre-built SageMaker LMI container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dfabe0a-04f8-486d-94ab-7d6066680954",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.25.0-deepspeed0.11.0-cu118\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    inference_image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-deepspeed\", region=region, version=\"0.25.0\"\n",
    "    )\n",
    "except ValueError:\n",
    "    inference_image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.25.0-deepspeed0.11.0-cu118\"\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905903de-a4b9-4f41-8cc2-564416ae5d5f",
   "metadata": {},
   "source": [
    "**Create the Tarball and then upload to S3 location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c005aa2e-ec1a-4ccf-8f39-67bcbabd0309",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'model.tar.gz': No such file or directory\n",
      "code_llama2_7b_fp16_smoothquant/\n",
      "code_llama2_7b_fp16_smoothquant/serving.properties\n"
     ]
    }
   ],
   "source": [
    "!rm model.tar.gz\n",
    "!tar czvf model.tar.gz code_llama2_7b_fp16_smoothquant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb83ba3b-2ea5-4297-8e85-f16dd4c7c13a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-521058400674/hf-large-model-djl/meta-llama/Llama2-7b-fp16-smoothquant/code/model.tar.gz'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama2_7b_fp16_smoothquant_s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, llama2_7b_fp16_smoothquant_s3_code_prefix)\n",
    "llama2_7b_fp16_smoothquant_s3_code_artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8e52615-159a-4a81-8558-00491460019b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-06 21:33:52        272 model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {llama2_7b_fp16_smoothquant_s3_code_artifact}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa56b75-356a-4ebf-bebd-7150521c95e9",
   "metadata": {},
   "source": [
    "### To create the endpoint the steps are:\n",
    "\n",
    "1. Create the Model using the Image container and the Model Tarball uploaded earlier\n",
    "2. Create the endpoint config using the following key parameters\n",
    "\n",
    "    a) Instance Type is ml.g5.2xlarge \n",
    "    \n",
    "    b) ContainerStartupHealthCheckTimeoutInSeconds is 3600 to ensure health check starts after the model is ready    \n",
    "3. Create the end point using the endpoint config created    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee8273-8009-4739-af67-79525b6882cd",
   "metadata": {},
   "source": [
    "#### Create the Model\n",
    "Use the image URI for the DJL container and the s3 location to which the tarball was uploaded.\n",
    "\n",
    "The container downloads the model into the `/tmp` space on the instance because SageMaker maps the `/tmp` to the Amazon Elastic Block Store (Amazon EBS) volume that is mounted when we specify the endpoint creation parameter VolumeSizeInGB. \n",
    "It leverages `s5cmd`(https://github.com/peak/s5cmd) which offers a very fast download speed and hence extremely useful when downloading large models.\n",
    "\n",
    "For instances like p4dn, which come pre-built with the volume instance, we can continue to leverage the `/tmp` on the container. The size of this mount is large enough to hold the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eac40dc4-c9c3-4af1-8d54-7b6742259266",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aim351-llama2-7b-smoothquant-2023-12-06-21-33-53-131\n",
      "Created Model: arn:aws:sagemaker:us-west-2:521058400674:model/aim351-llama2-7b-smoothquant-2023-12-06-21-33-53-131\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model_name = name_from_base(f\"aim351-llama2-7b-smoothquant\")\n",
    "print(model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": llama2_7b_fp16_smoothquant_s3_code_artifact,\n",
    "        \"Environment\": {\"MODEL_LOADING_TIMEOUT\": \"3600\"},\n",
    "    },\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "332e4a25-233f-4e75-aabb-0986c6c48f77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-west-2:521058400674:endpoint-config/aim351-llama2-7b-smoothquant-2023-12-06-21-33-53-131-config',\n",
       " 'ResponseMetadata': {'RequestId': 'fd403a7d-2c4f-40b6-9128-dc5784defad1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'fd403a7d-2c4f-40b6-9128-dc5784defad1',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '140',\n",
       "   'date': 'Wed, 06 Dec 2023 21:33:54 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.2xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": 3600,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b6a0ca5-3ee6-4793-83f4-d9565a7d06e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Endpoint: arn:aws:sagemaker:us-west-2:521058400674:endpoint/aim351-llama2-7b-smoothquant-2023-12-06-21-33-53-131-endpoint\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58385f65-b699-47d1-8449-268152c5b4d6",
   "metadata": {},
   "source": [
    "### Deploy Llama 2 7B model\n",
    "\n",
    "This step can take ~ 10 min or longer so please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f893ecb0-e0fe-450a-8bb1-5709d07b6e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-west-2:521058400674:endpoint/aim351-llama2-7b-smoothquant-2023-12-06-21-33-53-131-endpoint\n",
      "Status: InService\n",
      "CPU times: user 110 ms, sys: 23.2 ms, total: 133 ms\n",
      "Wall time: 7min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0ec2d7-6ef8-4383-ac50-6a4984af5c03",
   "metadata": {},
   "source": [
    "#### While you wait for the endpoint to be created, you can read more about:\n",
    "- [Deep Learning containers for large model inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-dlc.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aed031-b68c-4e23-8fa7-24483ec189d1",
   "metadata": {},
   "source": [
    "#### Leverage the Boto3 to invoke the endpoint. \n",
    "\n",
    "This is a generative model so we pass in a Text as a prompt and Model will complete the sentence and return the results.\n",
    "\n",
    "You can pass a prompt as input to the model. This done by setting inputs to a prompt. The model then returns a result for each prompt. The text generation can be configured using appropriate parameters.\n",
    "These parameters need to be passed to the endpoint as a dictionary of kwargs. Refer this documentation - https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig for more details.\n",
    "\n",
    "The below code sample illustrates the invocation of the endpoint using a text prompt and also sets some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d469839-8a73-467f-9c8b-a37792bd601d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template: [INST] <<SYS>>You are a helpful, respectful and honest assistant. \n",
      "Always answer as helpfully as possible, while being safe. \n",
      "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
      "Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \n",
      "If you don't know the answer to a question, please don't share false information.<</SYS>>\n",
      "What is AWS reinvent? Where and when does this happen every year?[/INST]\n"
     ]
    }
   ],
   "source": [
    "system_message = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. \n",
    "Always answer as helpfully as possible, while being safe. \n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \n",
    "If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "prompt = \"What is AWS reinvent? Where and when does this happen every year?\"\n",
    "\n",
    "prompt_template = f\"\"\"[INST] <<SYS>>{system_message}<</SYS>>\\n{prompt}[/INST]\"\"\"\n",
    "\n",
    "print(f\"Prompt template: {prompt_template}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff18eaa-9bb7-499e-84d3-b7415d5e646f",
   "metadata": {},
   "source": [
    "#### Inference Results\n",
    "\n",
    "Lets, invoke the model with the prompt defined above, body of the `invoke_endpoint()` parameter contains the prompt and the inference configurations including the following parameters\n",
    "\n",
    "- `max_new_tokens` The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "- `temperature`  The value used to modulate the next token probabilities.\n",
    "- `return_full_text` if True this will return our original prompt along with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb965d9a-1c1e-4b03-90a1-d5cf31fade99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: [INST] <<SYS>>You are a helpful, respectful and honest assistant. \n",
      "Always answer as helpfully as possible, while being safe. \n",
      "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
      "Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \n",
      "If you don't know the answer to a question, please don't share false information.<</SYS>>\n",
      "What is AWS reinvent? Where and when does this happen every year?[/INST]\n",
      "\n",
      "[INST] <<SYS>>AWS re:Invent is an annual conference hosted by Amazon Web Services (AWS) in Las Vegas, Nevada. \n",
      "The conference is held in late November and early December, and is attended by over 60,000 people. \n",
      "The conference features keynote speeches, breakout sessions, and networking opportunities for attendees. \n",
      "AWS re:Invent is a four-day event that includes keynote speeches, breakout sessions, and networking opportunities for attendees. \n",
      "The conference is held in late November and early December, and is attended by over 60,000 people. \n",
      "The conference features keynote speeches, breakout sessions, and networking opportunities for attendees. \n",
      "AWS re:Invent is a four-day event that includes keynote speeches, breakout sessions, and networking opportunities for attendees. \n",
      "The conference is held in late November and early December, and is attended by over 60,000 people. \n",
      "The conference features keynote speeches, breakout sessions, and networking\n",
      "CPU times: user 12.4 ms, sys: 3.7 ms, total: 16.1 ms\n",
      "Wall time: 5.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "body = json.dumps(\n",
    "        {\n",
    "            \"inputs\": prompt_template,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 256,\n",
    "                \"temperature\": 0.1\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=body,\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
    "output_response = output[0][\"generated_text\"]\n",
    "\n",
    "print(f\"Model Output: {output_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96330bed-fa20-409e-80ca-8ab28a9aa448",
   "metadata": {},
   "source": [
    "Great, you have successfully run inference on the prompt and got the output. Note that, first invocation is going to be slower beacuse the model has to be loaded to GPU memory. Subsequent invocations should be faster to get a response from Llama 2 7B Chat quantized model with SmoothQuant when it is already loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbe2436-9f06-46ae-9c48-fcf2a4c85cf4",
   "metadata": {},
   "source": [
    "### Calculate number of output tokens\n",
    "\n",
    "We will use AutoTokenizer from `transformers` package to load the tokenizer and calculate the number of token, we will use similar mechanism to benchmark the execution later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7a12152-8b2d-43ec-a2e0-f116255ffd2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c9dcbca55e41e0a2bc1ebd0f0ccbee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65bc86671eb24eaab1723beb92fd5d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a970353d8b48d6941ef1417e24fff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d797375749ed4662bbce819125b0e633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input tokens: 157\n",
      "Number of output tokens: 413\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_bnb = AutoTokenizer.from_pretrained(\"TheBloke/Llama-2-7B-fp16\")\n",
    "print(f\"Number of input tokens: {len(tokenizer_bnb.encode(prompt_template))}\")\n",
    "print(f\"Number of output tokens: {len(tokenizer_bnb.encode(output_response))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347133c8-5106-4127-a502-8644142541c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Benchmarking Llama 2 7B  SmoothQuant quantized model\n",
    "\n",
    "This following code is used to benchmark the speed of text generation for a given endpoint and tokenizer. We call the inference endpoint in a loop and keep track of the execution time and number of tokens generated to calculate tokens generated per second.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e894315-0fe3-4f22-8398-e31a9f2dbe8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "def benchmark_generation_speed(endpoint_name, tokenizer):\n",
    "    generation_time_list = []\n",
    "    num_generated_tokens_list = []\n",
    "\n",
    "    for i in tqdm(range(10)):\n",
    "        print(f\"Calling {endpoint_name} endpoint - #{i+1} time\")\n",
    "        start = time.time()\n",
    "        resp = smr_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=body,\n",
    "            ContentType=\"application/json\",\n",
    "        )\n",
    "        end = time.time()\n",
    "        generation_time_list.append(end - start)\n",
    "        output = json.loads(resp[\"Body\"].read().decode(\"utf-8\"))\n",
    "        output_resp = output[0][\"generated_text\"]\n",
    "        output_tokens = len(tokenizer.encode(output_resp))\n",
    "        num_generated_tokens_list.append(output_tokens)\n",
    "\n",
    "    total_tokens = sum(num_generated_tokens_list)\n",
    "    total_seconds = sum(generation_time_list)\n",
    "    total_tokens_per_second = total_tokens / total_seconds\n",
    "    print(f\"Generated {total_tokens} tokens using {total_seconds} seconds\")\n",
    "    print(f\"Generation speed: {total_tokens_per_second} tokens/s\") \n",
    "    \n",
    "    return total_tokens, total_seconds, total_tokens_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2397c47-aa76-41b4-a5b0-0d91429f28ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f45f2ffb684dd59ee75fcff13fa595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling aim351-llama2-7b-smoothquant-2023-12-06-21-33-53-131-endpoint endpoint - #1 time\n",
      "Calling aim351-llama2-7b-smoothquant-2023-12-06-21-33-53-131-endpoint endpoint - #2 time\n",
      "Calling aim351-llama2-7b-smoothquant-2023-12-06-21-33-53-131-endpoint endpoint - #3 time\n",
      "Calling aim351-llama2-7b-smoothquant-2023-12-06-21-33-53-131-endpoint endpoint - #4 time\n",
      "Calling aim351-llama2-7b-smoothquant-2023-12-06-21-33-53-131-endpoint endpoint - #5 time\n",
      "Calling aim351-llama2-7b-smoothquant-2023-12-06-21-33-53-131-endpoint endpoint - #6 time\n",
      "Calling aim351-llama2-7b-smoothquant-2023-12-06-21-33-53-131-endpoint endpoint - #7 time\n",
      "Calling aim351-llama2-7b-smoothquant-2023-12-06-21-33-53-131-endpoint endpoint - #8 time\n",
      "Calling aim351-llama2-7b-smoothquant-2023-12-06-21-33-53-131-endpoint endpoint - #9 time\n",
      "Calling aim351-llama2-7b-smoothquant-2023-12-06-21-33-53-131-endpoint endpoint - #10 time\n",
      "Generated 4130 tokens using 50.848403453826904 seconds\n",
      "Generation speed: 81.22182250520929 tokens/s\n"
     ]
    }
   ],
   "source": [
    "total_tokens, total_seconds, total_tokens_per_second = benchmark_generation_speed(endpoint_name, tokenizer_bnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ade42ad-9933-4f6b-8f20-58eb7781dc89",
   "metadata": {},
   "source": [
    "We can see that for Llama2 7B FP16 quantized model with SmoothQuant is able to proces ~80 tokens per second. You will be able to optimize for latency using quantization technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34a5722-4581-447f-af06-6cf853e7504c",
   "metadata": {},
   "source": [
    "### Congrats, you have suucessfully completed Lab2.\n",
    "\n",
    "Please proceed to clean up section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38886de1-a30b-41e3-b99d-b4454e37760b",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Run the code below to delete the endpoint and avoid any additional charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bb72ed7-ce42-480a-998f-809157fadb4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '67c5d582-6100-4060-970b-61a80c6ab5da',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '67c5d582-6100-4060-970b-61a80c6ab5da',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Wed, 06 Dec 2023 21:41:55 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Delete the end point\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82bca632-7305-4dcb-bf96-ae76337ab53a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '8786785d-98a4-461f-9bf0-853ec166c670',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '8786785d-98a4-461f-9bf0-853ec166c670',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Wed, 06 Dec 2023 21:41:55 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - In case the end point failed we still want to delete the model\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4a2424-ab00-4fd5-bbba-3e3b08e33af8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
