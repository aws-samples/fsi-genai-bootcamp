{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946d5a3d",
   "metadata": {},
   "source": [
    "# Serve Multiple Fine-Tuned LoRA Adapters on a single endpoint using SageMaker LMI DLC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35756ca1",
   "metadata": {},
   "source": [
    "This notebook illustrates the deployment of multiple fine-tuned LoRA adapters that leverages a single base model  on Amazon SageMaker using the DJL Serving Large Model Inference(LMI) container. LoRA, or Low Rank Adapters, is a parameter efficient fine-tuning(PEFT) technique, considerably diminishing the count of trainable parameters in contrast to conventional fine-tuning, yet delivering analogous or superior results. This aims to reduce the memory footprint during inference. For further insights into the LoRA technique, refer to this [paper](https://arxiv.org/abs/2106.09685).\n",
    "\n",
    "One of the principal advantages of LoRA lies in the facility to seamlessly integrate and detach the fine-tuned adapters from the base model, enabling cost-effective and feasible adapter interchange at runtime. This notebook exemplifies the deployment of a SageMaker endpoint with a unified base model and multiple LoRA adapters and demonstrates how to modify adapters to cater to varying requests.\n",
    "\n",
    "Given that LoRA adapters have a significantly smaller footprint compared to a base model (realistically being 100x-1000x smaller), itâ€™s plausible to deploy an endpoint with one base model and several LoRA adapters utilizing substantially fewer hardware resources than would be the case with an equivalent number of comprehensively fine-tuned models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e3b6d",
   "metadata": {},
   "source": [
    "### Install Packages and Import Dependencies\n",
    "\n",
    "First, lets install required libraries such as `sagemaker`, `boto3`, `awscli` and `huggingface_hub`. You can safely ingore any warnming related to running pip as `root` user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4ff2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub sagemaker boto3 awscli --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d2dbb4-d4e7-4251-8307-f20974bcaadc",
   "metadata": {},
   "source": [
    "### Imports and set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb06ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sagemaker.utils import name_from_base\n",
    "from huggingface_hub import snapshot_download, notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92199c38-348b-42dd-bdc8-fe9329506d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.session.Session()\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "s3_code_prefix = (\n",
    "    \"aim351-lab3/llama7b-lora\"  # folder within bucket where code artifact will go\n",
    ")\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37b203",
   "metadata": {},
   "source": [
    "### Download Model Artifacts and Upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1515c5",
   "metadata": {},
   "source": [
    "For the pupose of this lab, we use Llama-2-7b adapters from HuggingFace thats fine-tuned using PEFT LoRA technique. We will be deploying an endpoint with base LLaMA 7B model and 2 LoRA adapters. These are the models we will be using\n",
    "- Base Model: https://huggingface.co/huggyllama/llama-7b\n",
    "- Adapter tloen/alpaca-lora-7b\n",
    "- Adapter 22h/cabrita-lora-v0-1\n",
    "\n",
    "The `base_model` directory contains all the base model artifacts from the corresponding repository on the huggingface hub. These model artifacts are generated from the `model.save_pretrained()` method of huggingface's transformers library.\n",
    "\n",
    "Each of the adapters in the `adapters` directory contains the LoRA adapter artifacts. Typically there are two files: `adapter_model.bin` and `adapter_config.json` which are the adapter weights and adapter configuration respectively. These are typically obtained from the Peft library via the `PeftModel.save_pretrained()` method. In this example, the inference handler will register the adapters with names equivalent to their directory name, and we will use that name in the request to target a specific adapter.\n",
    "\n",
    "```\n",
    "|- base_model/\n",
    "|--- <base-model-artifacts>/\n",
    "|- adapters/\n",
    "|--- <lora1>/\n",
    "|-------- <lora1-model-artifacts>/\n",
    "|--- <lora2>/\n",
    "|-------- <lora2-model-artifacts>/\n",
    "|--- <lora3>/\n",
    "|-------- <lora3-model-artifacts>/\n",
    "|--- <lora_n>/\n",
    "|-------- <lora_n-model-artifacts>/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22997838-7c65-49ac-9ddb-8433123bb808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf lora-multi-adapter\n",
    "!mkdir -p lora-multi-adapter/adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5ee998-addf-47d3-a6c3-862d2dbb428b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snapshot_download(\"tloen/alpaca-lora-7b\", local_dir=\"lora-multi-adapter/adapters/eng_alpaca\", local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df8e31-6807-408b-a60c-a3294dde2e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snapshot_download(\"22h/cabrita-lora-v0-1\", local_dir=\"lora-multi-adapter/adapters/portuguese_alpaca\", local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89777de",
   "metadata": {},
   "source": [
    "## Creating Inference Handler and DJL Serving Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd1577",
   "metadata": {},
   "source": [
    "The following files cover the model server configuration (`serving.properties`) and custom inference handler (`model.py`). This configuration can be used as an example to write your own inference handler for different models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e988852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile lora-multi-adapter/serving.properties\n",
    "# Python engine is currently the only engine supported for multi adapter use-case\n",
    "engine=Python\n",
    "option.model_id=huggyllama/llama-7b\n",
    "option.adapters_path=adapters\n",
    "option.dtype=fp16\n",
    "option.entryPoint=model.py\n",
    "option.tensor_parallel_degree=4\n",
    "load_on_devices=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9f270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jinja2\n",
    "\n",
    "jinja_env = jinja2.Environment()\n",
    "# we plug in the appropriate model location into our `serving.properties` file based on the region in which this notebook is running\n",
    "template = jinja_env.from_string(Path(\"lora-multi-adapter/serving.properties\").open().read())\n",
    "Path(\"lora-multi-adapter/serving.properties\").open(\"w\").write(\n",
    "    template.render(s3_bucket=s3_bucket)\n",
    ")\n",
    "!pygmentize lora-multi-adapter/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a53d2-1772-455a-9b7a-32076f6a45a8",
   "metadata": {},
   "source": [
    "This section walks through how to serve Python based model with DJL Serving.\n",
    "\n",
    "1. Define Model\n",
    "To get started, implement a python source file named model.py as the entry point. DJL Serving will run your request by invoking a handle function that you provide. The handle function should have the following signature:\n",
    "\n",
    "    ```def handle(inputs: Input)```\n",
    "    \n",
    "2. If there are other packages you want to use with your script, you can include a requirements.txt file in the same directory with your model file to install other dependencies at runtime. A requirements.txt file is a text file that contains a list of items that are installed by using pip install. You can also specify the version of an item to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f9a2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile lora-multi-adapter/model.py\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import os\n",
    "from djl_python.inputs import Input\n",
    "from djl_python.outputs import Output\n",
    "import logging\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def generate_prompt(instruction, input=None):\n",
    "    if input:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "        Write a response that appropriately completes the request. ### Instruction: {instruction} ### Input: {input} \n",
    "        ### Response:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the \n",
    "        request.### Instruction: {instruction} ### Response:\"\"\"\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "        instruction,\n",
    "        adapters,\n",
    "        input=None,\n",
    "        max_new_tokens=64,\n",
    "        **kwargs,\n",
    "):\n",
    "    prompts = []\n",
    "    for inp in instruction:\n",
    "        prompts.append(generate_prompt(inp, input))\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = inputs[\"input_ids\"].to(torch.cuda.current_device())\n",
    "    attention_mask = inputs[\"attention_mask\"].to(torch.cuda.current_device())\n",
    "    generation_config = GenerationConfig(num_beams=1, do_sample=False)\n",
    "\n",
    "    logging.info(f\"using adapters: {adapters}\")\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            adapters=adapters,\n",
    "            generation_config=generation_config,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    output = tokenizer.batch_decode(generation_output, skip_special_tokens=True)\n",
    "    return output\n",
    "\n",
    "\n",
    "def load_model(model_id):\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = '[PAD]'\n",
    "    logging.info(f\"Loaded Base Model {model_id}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def register_adapter(inputs: Input):\n",
    "    \"\"\"\n",
    "    Registers lora adapter with the model.\n",
    "    \"\"\"\n",
    "    global model\n",
    "    adapter_name = inputs.get_property(\"name\")\n",
    "    adapter_model_id_or_path = inputs.get_property(\"src\")\n",
    "    logging.info(\n",
    "        f\"Registering adapter {adapter_name} from {adapter_model_id_or_path}\")\n",
    "    if isinstance(model, PeftModel):\n",
    "        model.load_adapter(adapter_model_id_or_path, adapter_name)\n",
    "    else:\n",
    "        model = PeftModel.from_pretrained(model,\n",
    "                                           adapter_model_id_or_path,\n",
    "                                           adapter_name)\n",
    "\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model, tokenizer\n",
    "    if not model:\n",
    "        properties = inputs.get_properties()\n",
    "        model_id = properties.get(\"model_id\")\n",
    "        model, tokenizer = load_model(model_id)\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "\n",
    "\n",
    "    json_inputs = inputs.get_as_json()\n",
    "    sentence = json_inputs.get(\"inputs\")\n",
    "    adapters = json_inputs.get(\"adapters\", [])\n",
    "    generation_kwargs = json_inputs.get(\"parameters\", {})\n",
    "    outputs = evaluate(sentence, adapters, **generation_kwargs)\n",
    "\n",
    "    return Output().add_as_json(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b559cd9-b85e-456d-8061-1585f1412f3f",
   "metadata": {},
   "source": [
    "DJL Serving supports model artifacts in model directory, .zip or .tar.gz format. To package model artifacts in a .tar.gz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c45680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -f model.tar.gz\n",
    "!rm -rf lora-multi-adapter/.ipynb_checkpoints\n",
    "!tar czvf model.tar.gz -C lora-multi-adapter ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6dca13",
   "metadata": {},
   "source": [
    "## Create SageMaker Model and Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a08dd-ad8d-42eb-8a0a-6b0f6e148d5a",
   "metadata": {},
   "source": [
    "SageMaker expects the model artifact to be uploaded in S3, so we upload the tar artiface to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab57364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_artifact_accelerate = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "s3_code_artifact_accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a884805a-1a29-441f-a15f-8f44a9e3e01f",
   "metadata": {},
   "source": [
    "We then proceed to retrieve the LMI container image and create the model using the S3 Code artifacrs and container image URI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc7b9a1-da72-4146-bb21-6e6897484730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    inference_image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-deepspeed\", region=region, version=\"0.25.0\"\n",
    "    )\n",
    "except ValueError:\n",
    "    inference_image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.25.0-deepspeed0.11.0-cu118\"\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e67228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name_acc = name_from_base(f\"llama7b-lora\")\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name_acc,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": inference_image_uri, \n",
    "                      \"ModelDataUrl\": s3_code_artifact_accelerate,\n",
    "                      \"Environment\": {\"ENABLE_ADAPTERS_PREVIEW\": \"true\"},\n",
    "                     })\n",
    "model_arn = create_model_response[\"ModelArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c085daa-4f75-447e-ad6b-5168115da1bc",
   "metadata": {},
   "source": [
    "We will be deploying this endpoint to a g5.2xlarge instnace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8699f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name_acc}-config\"\n",
    "endpoint_name = f\"{model_name_acc}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name_acc,\n",
    "            \"InstanceType\": \"ml.g5.4xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": 3600,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600,\n",
    "            # \"VolumeSizeInGB\": 512\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a346666f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ebbd3d-66b2-49cd-8b12-2ce6a37d8d22",
   "metadata": {},
   "source": [
    "### Deploy Llama 2 7B model\n",
    "\n",
    "This step can take ~ 10 min or longer so please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ad681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7680b38a",
   "metadata": {},
   "source": [
    "## Make Inference Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552ac360",
   "metadata": {},
   "source": [
    "We show how you can make requests targetting each of the adapters configured in the endpoint, as well as requests targetting just the base model with no adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5f50dc",
   "metadata": {},
   "source": [
    "Inference Request targetting the 'portuguese_alpaca' and 'eng_alpaca' adapter (named eng_alpaca based on the directory name of the adapter). This invocation is a batched request for mulitple adapter in a single call, this hows support for heterogenous batched multi-lora request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84948200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps({\"inputs\": [\"what is aws reinvent\", \"translate to english: Estou na AWS re:invent e estou gostando\", \"Does aws reinvent happen in Las Vegas?\"],\n",
    "                     \"adapters\": [\"eng_alpaca\", \"portuguese_alpaca\", \"eng_alpaca\"]}),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response = response_model[\"Body\"].read().decode(\"utf8\").replace(\"[\",\"\").replace(\"]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc854039-59e2-414c-ac10-30dced7ba728",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7883ea4",
   "metadata": {},
   "source": [
    "You may want to consider post processing the response depending on your requirements. Below is an example of inference request targetting single portuguese_alpaca adapter (named portuguese_alpaca based on the direcotry name of the adapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06b463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": [\"Translate this sentence from portugese to english: Estou na AWS re:invent e estou gostando\"],\n",
    "            \"adapters\": [\"portuguese_alpaca\"],\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d434c23",
   "metadata": {},
   "source": [
    "### Congrats, you have successfully completed Lab3.\n",
    "\n",
    "Please proceed to clean up section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef62f3e",
   "metadata": {},
   "source": [
    "## Clean up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baadd61d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e9d6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54afe9b0-7ab5-49d1-b392-f2b061c3e186",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (Optional)Fine-tune and deploy Llama-2-7B multi-LoRA adapter model\n",
    "\n",
    "Learn how to fine-tune LoRA on your data set and set up multi-adaper inference using the optional notebooks included in this lab\n",
    "\n",
    "#### Fine-tuning\n",
    "\n",
    "Following are the PEFT LoRA configuration set during the fine-tuning process. The Llama7B model is fine-tuned with different dataset and 3 different adapter were created and loaded in S3 location.\n",
    "\n",
    "The key parameters are:\n",
    "\n",
    "- task_type: Sets this as a Causal LM task (generating text auto-regressively)\n",
    "- inference_mode: False indicates this is for training, not inference/evaluation\n",
    "- r: The rank for the Lora decomposition (defaults to 8)\n",
    "- lora_alpha: The alpha parameter for Lora (defaults to 32) \n",
    "- lora_dropout: The dropout rate for Lora (defaults to 0.05)\n",
    "- target_modules: The transformer modules to apply Lora to. Here it is commented out, so Lora will be applied to all modules.\n",
    "\n",
    "For more details on fine-tuning process, please refer `(optional)-llama7b-lora-fine-tune.ipynb` notebook and assiciated scripts\n",
    "\n",
    "#### Multi-LoRA Inference\n",
    "\n",
    "For more details on fine-tuning process, please refer `(optional)-llama7b-lora-adapter.ipynb` notebook and assiciated scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52446b72-24eb-4b20-8720-9ddfa0669a76",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
