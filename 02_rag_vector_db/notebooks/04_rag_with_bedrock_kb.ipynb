{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26302d4",
   "metadata": {},
   "source": [
    "# Fully managed RAG with Amazon Bedrock Knowledge Bases\n",
    "> *This notebook should work well with the **`Python 3 (ipykernel)`** kernel in SageMaker Studio on ml.t3.medium instance*\n",
    "\n",
    "In the prior notebook, we built a RAG solution from scratch using Amazon OpenSearch Service and abstractions provided by the `langchain` library. In this notebook we will looks at a fully managed alternative that will handle the entire pipeline for us including:\n",
    "- **Document ingestion**: Sourcing documents from S3 and ingesting them into a vector database\n",
    "- **Document processing**: Extracting text from documents and chunking them into smaller pieces\n",
    "- **Vectorization**: Converting text into vector embeddings\n",
    "- **Retrieval**: Searching for relevant documents based on a query\n",
    "- **Generation**: Generating a response based on the retrieved documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f749aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce6c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = \"../..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.environment_validation import validate_environment, validate_model_access\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a03b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_models = [\n",
    "    \"amazon.titan-embed-text-v1\",\n",
    "    \"amazon.titan-embed-text-v2:0\",\n",
    "    \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "]\n",
    "validate_model_access(required_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c25378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import boto3\n",
    "import time\n",
    "from rag_utils.kb_utils import upload_document, create_kb, create_data_source, get_collection_data\n",
    "from rich import print as rprint\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "boto3_session = boto3.Session()\n",
    "REGION = boto3_session.region_name\n",
    "BEDROCK_AGENT_CLIENT = boto3.client(\"bedrock-agent-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8399b66",
   "metadata": {},
   "source": [
    "Before the documents can be ingested, they must first be uploaded to an S3 bucket. The following code will upload the documents to S3 and run an ls command to confirm the upload.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_governance_docs_path = Path(\"../data/model_risk\")\n",
    "s3_docs_path = upload_document(doc_path=model_governance_docs_path, s3_prefix=\"model-risk-docs\")\n",
    "!aws s3 ls {s3_docs_path} --recursive --human-readable --summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e404a",
   "metadata": {},
   "source": [
    "Before a Knowledge Base can be created, we need to configure a Vector Database (Vector DB) to store and retrieve the document embeddings. There are a number of supported options as documented [here](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-setup.html). For out purposes, the workshop already includes a pre-provisioned [Amazon OpenSearch Serverless Collection](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html) that we can use. The function below will look up the endpoint and arn for the OpenSearch collection and return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_endpoint, collection_arn = get_collection_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5170be43",
   "metadata": {},
   "source": [
    "We can now create the Knowledge Base and ingest the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "retries = 3\n",
    "while retries > 0:\n",
    "    try:\n",
    "        kb_id = create_kb(\n",
    "            collection_arn=collection_arn,\n",
    "            collection_endpoint=collection_endpoint,\n",
    "            index_name=\"model-risk-docs\",\n",
    "            kb_name=\"model-risk-docs\",\n",
    "            kb_description=\"Model Risk Documents\",\n",
    "        )\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating KB: {e} retrying...\")\n",
    "        time.sleep(10)\n",
    "        retries -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da2ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data source for the uploaded documents\n",
    "data_source_id = create_data_source(kb_id=kb_id, s3_path=s3_docs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e35c1c1",
   "metadata": {},
   "source": [
    "We'll create a helper function to query the KB. The function will format the output to reteurn the generated response along with specific references to the documents that were used to generate the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_knowledge_base(kb_id, query_text, session_id=None):\n",
    "    params = {\n",
    "        'input': {'text': query_text},\n",
    "        'retrieveAndGenerateConfiguration': {\n",
    "            'type': 'KNOWLEDGE_BASE',\n",
    "            'knowledgeBaseConfiguration': {\n",
    "                'knowledgeBaseId': kb_id,\n",
    "                'modelArn': f'arn:aws:bedrock:{REGION}::foundation-model/anthropic.claude-3-5-haiku-20241022-v1:0'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    if session_id is not None:\n",
    "        params['sessionId'] = session_id\n",
    "\n",
    "    response = BEDROCK_AGENT_CLIENT.retrieve_and_generate(**params)\n",
    "    return response\n",
    "\n",
    "def print_response(response):\n",
    "\n",
    "    ref_number = 0\n",
    "    ref_offset = 1\n",
    "    output = response[\"output\"][\"text\"]\n",
    "    references = []\n",
    "\n",
    "    for citation in response[\"citations\"]:\n",
    "\n",
    "        citation_end = citation[\"generatedResponsePart\"][\"textResponsePart\"][\"span\"][\"end\"]\n",
    "\n",
    "        citation_string = \"[\"\n",
    "\n",
    "        for n, reference in enumerate(citation[\"retrievedReferences\"]):\n",
    "            ref_location = reference[\"location\"][\"s3Location\"][\"uri\"]\n",
    "            ref_page_number = reference[\"metadata\"][\"x-amz-bedrock-kb-document-page-number\"]\n",
    "            ref_number += 1\n",
    "            references.append(f\"[{ref_number}]: {ref_location} (page {int(ref_page_number)})\")\n",
    "\n",
    "            if n == 0:\n",
    "                citation_string += f\"{ref_number}\"\n",
    "            else:\n",
    "                citation_string += f\", {ref_number}\"\n",
    "        \n",
    "        citation_string += \"]\"\n",
    "\n",
    "        output = output[:citation_end + ref_offset] + citation_string + output[citation_end + ref_offset:]\n",
    "        ref_offset += len(citation_string)\n",
    "    \n",
    "    reference_str = \"\\n\".join(references)\n",
    "    rprint(output)\n",
    "    rprint(reference_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30a4b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"What are the best practices for model governance?\"\n",
    "response = query_knowledge_base(kb_id=kb_id, query_text=query_text)\n",
    "session_id = response[\"sessionId\"]\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af56604",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"What are some example risk that I should document for a credit risk model?\"\n",
    "response = query_knowledge_base(kb_id=kb_id, query_text=query_text, session_id=session_id)\n",
    "\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6733ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"How about for a fraud detection model?\"\n",
    "response = query_knowledge_base(kb_id=kb_id, query_text=query_text, session_id=session_id)\n",
    "\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde816b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
