{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall autogluon-multimodal autogluon-timeseries -y\n",
    "!pip install -Uqq torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.00 --index-url https://download.pytorch.org/whl/cu126\n",
    "!FLASH_ATTENTION_SKIP_CUDA_BUILD=TRUE pip install -Uqq --no-cache-dir \"unsloth[cu126-ampere-torch260] @ git+https://github.com/unslothai/unsloth.git\" \n",
    "!pip install -Uqq accelerate bitsandbytes\n",
    "!pip install -U tensorflow\n",
    "!pip install -U awscli\n",
    "!pip uninstall flash_attn -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune model for better RAG\n",
    "In this notebook we will fine-tune the `Mistral 7B` model on the previously generated dataset. There are several reasons why we might want to fine-tune a model:\n",
    "- Our data is domain specific and is distinct from the data the model was trained on.\n",
    "- The model is not performing well on the task we are interested in.\n",
    "- We want to better align the models responses with our expectations.\n",
    "\n",
    "### Approach\n",
    "We will follow a variation of the approach proposed in the Retrieval Augmented Fine Tuning [RAFT](https://arxiv.org/pdf/2403.10131). The main idea is we will create a fine-tuning dataset that is comprised of the question, relevant and irrelevant contexts, and the answer. We will then fine-tune the model on this dataset. The hope is that the model will learn to better distinguish between relevant and irrelevant contexts when formulating the response. This better simulates the real-world scenario where the retriever will likely return both relevant and irrelevant contexts and we need the model to be able to distinguish between the two.\n",
    "\n",
    "To speed up training, we will use the [unsloth](https://github.com/unslothai/unsloth) library combined with Hugging Face TRL as described in this [blog post](https://huggingface.co/blog/unsloth-trl). This would allows us to complete more fine-tuning within a limited amount of time than would have been possible in this environment. \n",
    "\n",
    "### Why combine RAG with fine-tuning?\n",
    "You may wonder why we are fine-tuning the model for RAG rather than fine-tuning on the question and answer task directly. One reason is that this approach trains the model to reason over our custom domain. This allows us to adapt the model to a wide variety of tasks besides question answering. For example, we could use the model to summarize specific regulations or extract entities. Another reason is that this approach would allows us to deal with changing documents without having to retrain the model. In our example the Banking Regulations change fairly frequently as we cas see in this [Timeline](https://www.ecfr.gov/recent-changes?search%5Bdate%5D=current&search%5Bhierarchy%5D%5Btitle%5D=12) view. It would be cost prohibitive and impractical to retrain the model every time the regulations change. However by tuning the model to reason over the documents, we can simply ingest the latest documents into the vector store and the model should be able to provide accurate and up to date answers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will download the pre-trained model from a SageMaker Jumpstart S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_path = \"mistral-7b-instruct\"\n",
    "if not Path(local_model_path).exists():\n",
    "    !aws s3 cp --recursive s3://jumpstart-cache-prod-{region}/huggingface-llm/huggingface-llm-mistral-7b-instruct/artifacts/inference-prepack/v2.0.0/ {local_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the model using unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 8192\n",
    "load_in_4bit = True\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = local_model_path, \n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of parameter efficient fine-tuning (PEFT) which will train a small number of additional parameter to adapt the model for our task. See [here](https://github.com/huggingface/peft) for more details. \n",
    "\n",
    "The parameters below such as `r`, `lora_alpha`, and `lora_dropout` can be tuned to improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",  \n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 42,\n",
    "    use_rslora = True,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will load our json lines dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "train_dataset_path = Path(\"data/prepared_data/prepared_data_train.jsonl\")\n",
    "test_dataset_path = Path(\"data/prepared_data/prepared_data_test.jsonl\")\n",
    "train_dataset = load_dataset(\"json\", data_files = [train_dataset_path.as_posix()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to prepare our data as per the RAFT technique. We'll use a variation of the [Mistral prompt](https://www.promptingguide.ai/models/mistral-7b#chat-template-for-mistral-7b-instruct) as our template to combine the question, relevant and irrelevant contexts, and the answer. \n",
    "\n",
    "The number of irrelevant contexts or distractor documents as they are referred to in the RAFT paper is determined by `DISTRACT_DOCS` variable. So with `DISTRACT_DOCS=2` we will include 2 irrelevant documents for each question in addition to the correct document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "prompt_template = \"[INST] You are a Banking Regulations expert.\\nGiven this context\\nCONTEXT\\n{context}\\n Answer this question\\nQuestion: {question} [/INST] {response} \"\n",
    "\n",
    "DISTRACT_DOCS = 2\n",
    "\n",
    "BOS_TOKEN = tokenizer.bos_token\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    contexts = examples[\"context\"]\n",
    "    questions = examples[\"question\"]\n",
    "    responses  = examples[\"answer\"]\n",
    "    texts = []\n",
    "    for context, question, response in zip(contexts, questions, responses):\n",
    "        distractor_contexts = [train_dataset[\"train\"][randint(0, train_dataset[\"train\"].num_rows -1)] for _ in range(DISTRACT_DOCS)]\n",
    "        \n",
    "        context_docs = [context] + [doc[\"context\"] for doc in distractor_contexts]\n",
    "        context = \"\\n\".join(context_docs)\n",
    "        \n",
    "        text = BOS_TOKEN + prompt_template.format(context=context, question=question, response=response) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "train_dataset = train_dataset[\"train\"].map(formatting_prompts_func, batched = True)\n",
    "train_dataset = train_dataset.shuffle(seed = 3407)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we configure the training parameters and start the training process using the [Hugging Face TRL](https://github.com/huggingface/trl) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow_utils\n",
    "import mlflow\n",
    "\n",
    "mlflow_config_path = Path(\"mlflow_config.json\")\n",
    "if not mlflow_config_path.exists():\n",
    "    print(\n",
    "        \"No MLFlow configuration found. Please run the first notebook to set up MLFlow.\"\n",
    "    )\n",
    "    mlflow_available = False\n",
    "else:\n",
    "    mlflow_config = json.loads(mlflow_config_path.read_text())\n",
    "    server_status = mlflow_utils.check_server_status(\n",
    "        mlflow_config[\"tracking_server_name\"]\n",
    "    )\n",
    "    if server_status[\"IsActive\"] == \"Active\":\n",
    "        print(\n",
    "            f'MLFlow server is available. The current status is: {server_status[\"TrackingServerStatus\"]}'\n",
    "        )\n",
    "        mlflow_available = True\n",
    "        mlflow.set_tracking_uri(mlflow_config[\"tracking_server_arn\"])\n",
    "        mlflow.set_experiment(\"fine-tuning-banking-regulations\")\n",
    "    else:\n",
    "        mlflow_available = False\n",
    "        print(\n",
    "            f'MLFlow server is not available. The current status is: {server_status[\"TrackingServerStatus\"]}'\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "if mlflow_available:\n",
    "    pre_signed_url = mlflow_utils.create_presigned_url(mlflow_config[\"tracking_server_name\"])\n",
    "    display(Markdown(f\"Our experiment results will be logged to MLFlow. You can view them from the [MLFlow UI]({pre_signed_url})\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to= \"mlflow\" if mlflow_available else None,\n",
    "        run_name=\"fine-tuning\"\n",
    "    )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = True, # Can make training 5x faster for short sequences.\n",
    "    args = training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mlflow_available:\n",
    "    mlflow.log_params(training_args.to_dict())\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Inference\n",
    "To do a quick test of the model we can run some local inference to make sure that the model is working as expected. We'll perform further validation as part of our RAG pipeline in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "test_dataset = load_dataset(\"json\", data_files = [test_dataset_path.as_posix()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick and index from 0 to 399 to test the model\n",
    "idx = 125\n",
    "test_question = test_dataset[\"train\"][idx][\"question\"]\n",
    "test_context = test_dataset[\"train\"][idx][\"context\"]\n",
    "test_answer = test_dataset[\"train\"][idx][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "inference_template = \"[INST] You are a Banking Regulations expert.\\nGiven this context\\nCONTEXT\\n{context}\\n Answer this question\\nQuestion: {question} [/INST]\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    inference_template.format(context=test_context, question=test_question)\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 2000)\n",
    "answer = tokenizer.batch_decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens = True)\n",
    "\n",
    "print(\"Question: \", test_question)\n",
    "print(\"\\nGenerated Answer: \", test_answer)\n",
    "print(\"\\nGround Truth Answer: \", test_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can register the model into the [MLFLow model registry](https://mlflow.org/docs/latest/model-registry.html). A model registry is a critical component of the ML lifecycle that allows you to manage the full lifecycle of a model, from experimentation to deployment. We can use the MLFlow model registry to track the model version, stage the model for deployment, and manage the model lifecycle. MLFlow registry can automatically sync with the [SageMaker Model Registry](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow-track-experiments-model-registration.html) to provide a seamless experience for model deployment and MLOps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models import infer_signature\n",
    "\n",
    "# MLflow infers schema from the provided sample input/output/params\n",
    "signature = infer_signature(\n",
    "    model_input=inference_template.format(context=test_context, question=test_question),\n",
    "    model_output=test_answer,\n",
    ")\n",
    "\n",
    "mlflow.transformers.log_model(\n",
    "    transformers_model={\"model\": trainer.model, \"tokenizer\": tokenizer},\n",
    "    task=\"text-generation\",\n",
    "    prompt_template=\"[INST] {prompt} [/INST]\",\n",
    "    signature=signature,\n",
    "    artifact_path=\"banking-regulations-adapter\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would normally use the model registry as part of an MLOps deployment pipeline. However, for the purposes of this workshop we will save the model to a local directory for use deployment in the next notebook. There are several options for saving the model including saving only the fine-tune adapter, merging the adapter with the model and saving the entire model, and saving quantized model or half precision model. Saving just the adapter is the most space efficient option and would enable efficient multi-tenant serving as described in this [blog post](https://aws.amazon.com/blogs/machine-learning/easily-deploy-and-manage-hundreds-of-lora-adapters-with-sagemaker-efficient-multi-adapter-inference/) and would thus be the recommended approach. To simplify the scope of this workshop, we'll save the entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this notebook we saw how we can fine-tune a model for RAG using the RAFT technique. We also saw how we can use the unsloth library to speed up the fine-tuning process. In the next notebook we will deploy the model as a REST API endpoint using SageMaker Hosting Services."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
