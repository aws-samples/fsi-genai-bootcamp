{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Baseline Retrieval Augment Generation Solution\n",
    "In this notebook we will build an initial solution that will utilize a pre-trained model augmented with a contextual data from a vector store retriever. At a high level, the solution will work as follows:\n",
    "- Based on a user's query, we will retrieve the top-k most similar documents from the vector store.\n",
    "- Provide the relevant documents as part of the prompt to the model along with the user's question\n",
    "- Generate the answer using the model\n",
    "\n",
    "![Basic RAG](images/chatbot_lang.png)\n",
    "\n",
    "We'll evaluate several aspects of the solution including:\n",
    "- The accuracy of the retrieved context\n",
    "- The quality of the generated answer\n",
    "\n",
    "These metrics will help determine whether a solution using purely pre-trained models is viable or whether we need to consider more complex strategies or fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = \"../..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.environment_validation import validate_environment, validate_model_access\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_models = [\n",
    "    \"amazon.titan-embed-text-v2:0\",\n",
    "    \"mistral.mixtral-8x7b-instruct-v0:1\",\n",
    "    \"mistral.mistral-7b-instruct-v0:2\",\n",
    "    \"us.amazon.nova-lite-v1:0\"\n",
    "]\n",
    "validate_model_access(required_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion\n",
    "The prepared datasets have been split into training and validation sets. We will load documents associated with both sets into a vector store for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from rich import print as rprint\n",
    "from IPython.display import display, Markdown\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import langchain\n",
    "from langchain_core.documents import Document\n",
    "from langchain_aws.chat_models import ChatBedrockConverse\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import boto3\n",
    "\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "data_path = Path(\"data/prepared_data\")\n",
    "train_data = (data_path / \"prepared_data_train.jsonl\").read_text().splitlines()\n",
    "test_data = (data_path / \"prepared_data_test.jsonl\").read_text().splitlines()\n",
    "\n",
    "doc_ids = []\n",
    "documents = []\n",
    "\n",
    "# Create a list of LangChain documents that can then be used to ingest into a vector store\n",
    "\n",
    "for record in chain(train_data, test_data):\n",
    "    json_record = json.loads(record)\n",
    "    if json_record[\"ref_doc_id\"] not in doc_ids:\n",
    "        doc_ids.append(json_record[\"ref_doc_id\"])\n",
    "        doc = Document(page_content=json_record[\"context\"], metadata=json_record[\"section_metadata\"])\n",
    "        documents.append(doc)\n",
    "\n",
    "print(f\"Loaded {len(documents)} sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow_utils\n",
    "import mlflow\n",
    "\n",
    "mlflow_config_path = Path(\"mlflow_config.json\")\n",
    "if not mlflow_config_path.exists():\n",
    "    rprint(\n",
    "        \"No MLFlow configuration found. Please run the first notebook to set up MLFlow.\"\n",
    "    )\n",
    "else:\n",
    "    mlflow_config = json.loads(mlflow_config_path.read_text())\n",
    "    server_status = mlflow_utils.check_server_status(\n",
    "        mlflow_config[\"tracking_server_name\"]\n",
    "    )\n",
    "    if server_status[\"IsActive\"] == \"Active\":\n",
    "        rprint(\n",
    "            f'MLFlow server is available. The current status is: {server_status[\"TrackingServerStatus\"]}'\n",
    "        )\n",
    "        mlflow_available = True\n",
    "        mlflow.set_tracking_uri(mlflow_config[\"tracking_server_arn\"])\n",
    "    else:\n",
    "        mlflow_available = False\n",
    "        rprint(\n",
    "            f'MLFlow server is not available. The current status is: {server_status[\"TrackingServerStatus\"]}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will initialize the embedding model that will be used to vectorize the documents and queries. We will use the `amazon.titan-embed-text-v2:0` model for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_session=boto3.session.Session()\n",
    "\n",
    "bedrock_runtime = boto3_session.client(\"bedrock-runtime\")\n",
    "bedrock_client = boto3_session.client(\"bedrock\")\n",
    "\n",
    "embedding_modelId = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "embed_model = BedrockEmbeddings(\n",
    "    model_id=embedding_modelId,\n",
    "    model_kwargs={\"dimensions\": 1024, \"normalize\": True},\n",
    "    client=bedrock_runtime,\n",
    ")\n",
    "\n",
    "query = \"Do I really need to fine-tune the large language models?\"\n",
    "response = embed_model.embed_query(query)\n",
    "rprint(f\"Generated an embedding with {len(response)} dimensions. Sample of first 10 dimensions:\\n\", response[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documents can now be ingested into a vector store. We will utilize a local vector store backed by the `faiss` library for this purpose. In production scenarios, a more scalable solution like OpenSearch or pgvector should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_file = \"baseline_rag_vec_db.pkl\"\n",
    "\n",
    "if not Path(vector_store_file).exists():\n",
    "    rprint(f\"Vector store file {vector_store_file} does not exist. Will create a new vector store.\")\n",
    "    CREATE_NEW = True\n",
    "else:\n",
    "    rprint(f\"Vector store file {vector_store_file} already exists and will be reused. Delete it or change the file name above to if you wish to create a new vector store.\")\n",
    "    CREATE_NEW = False \n",
    "\n",
    "if CREATE_NEW:\n",
    "    vec_db = FAISS.from_documents(documents, embed_model)\n",
    "    pickle.dump(vec_db.serialize_to_bytes(), open(vector_store_file, \"wb\"))\n",
    "    \n",
    "else:\n",
    "    if not Path(vector_store_file).exists():\n",
    "        raise FileNotFoundError(f\"Vector store file {vector_store_file} not found. Set CREATE_NEW to True to create a new vector store.\")\n",
    "    \n",
    "    vector_db_buff = BytesIO(pickle.load(open(vector_store_file, \"rb\")))\n",
    "    vec_db = FAISS.deserialize_from_bytes(serialized=vector_db_buff.read(), embeddings=embed_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the retrieval performance\n",
    "Before moving on to the generation step, we should validate the performance of the retriever. The large language model will not be able to generate accurate answers if the retrieved context is not relevant. We will evaluate the retriever using the validation set. The prepared validation set contains 400 questions along with relevant contexts. For each question, we have the unique document id of the relevant context. So our evaluation is simple: we will retrieve the top-k documents for each question and check if the relevant context is present in the top-k results. We will then calculate the recall or Hit Rate of the retriever. Additionally we'll compute the MRR (Mean Reciprocal Rank) metric. The MRR is the average of the reciprocal ranks of the first relevant document. For example, if we retrieve 5 documents (k=5) and the relevant document is ranked 2nd, the reciprocal rank would be 1/2. We calculate the reciprocal rank for each question and then take the average to get the MRR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = (data_path / \"prepared_data_test.jsonl\").read_text().splitlines()\n",
    "retriever_evaluation_data = []\n",
    "\n",
    "# we only need the ref_doc_id and question from the test data\n",
    "\n",
    "for record in test_data:\n",
    "    json_record = json.loads(record)\n",
    "    retriever_evaluation_data.append({\"ref_doc_id\":json_record[\"ref_doc_id\"], \"question\":json_record[\"question\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mlflow_available:\n",
    "    pre_signed_url = mlflow_utils.create_presigned_url(mlflow_config[\"tracking_server_name\"])\n",
    "    display(Markdown(f\"Our experiment results will be logged to MLFlow. You can view them from the [MLFlow UI]({pre_signed_url})\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3 # number of documents to retrieve\n",
    "faiss_retriever = vec_db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "\n",
    "correct = 0\n",
    "reciprocal_rank = 0\n",
    "num_examples = 400 # Number of examples to evaluate\n",
    "for i, eval_data in enumerate(retriever_evaluation_data[:num_examples]):\n",
    "    returned_docs = faiss_retriever.invoke(eval_data[\"question\"])\n",
    "    returned_doc_ids = [doc.metadata[\"unique_id\"] for doc in returned_docs]\n",
    "    if eval_data[\"ref_doc_id\"] in returned_doc_ids:\n",
    "        correct += 1\n",
    "        reciprocal_rank += 1 / (returned_doc_ids.index(eval_data[\"ref_doc_id\"]) + 1)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "hit_rate = correct / num_examples\n",
    "mrr = reciprocal_rank / num_examples\n",
    "\n",
    "print(f\"Hit rate @k={k}: {hit_rate}\")\n",
    "print(f\"MRR @k={k}: {mrr}\")\n",
    "\n",
    "if mlflow_available:\n",
    "    mlflow.set_experiment(\"Retriever Evaluation\")\n",
    "    with mlflow.start_run(run_name=\"baseline_retriever\"):\n",
    "        mlflow.log_param(\"retriever\", \"FAISS\")\n",
    "        mlflow.log_param(\"k\", k)\n",
    "        mlflow.log_metric(\"hit_rate\", hit_rate)\n",
    "        mlflow.log_metric(\"mrr\", mrr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation results above may vary but we should see a hit rate of over 0.92 and an MRR of over 0.85. These results are quite good and indicate that the retriever is able to find the relevant context for most questions. If this was not the case, then using a different embedding model or fine-tuning the retriever would be possible options to consider. A number of libraries exist that can be used to fine-tune or train a custom embedding model for retrieval including:\n",
    "- [sentence-transformers](https://www.sbert.net/docs/sentence_transformer/training_overview.html)\n",
    "- [RAGatouille](https://github.com/bclavie/RAGatouille)\n",
    "\n",
    "There are other ways to improve the retriever performance such as using hybrid search that combines both dense and sparse retrieval methods. \n",
    "\n",
    "For example below, we can improve the performance of the above retriever by ensembling it with a sparse retriever like BM25. This tends to work well with domain specific datasets as it combines the strengths of keyword search with semantic search. We'll use langchain's [EnsembleRetriever](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/ensemble/) to combine the dense retriever with BM25. However many vector dbs offer hybrid search capabilities out of the box such as  [OpenSearch](https://opensearch.org/docs/latest/search-plugins/hybrid-search/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "bm_25 = BM25Retriever.from_documents(documents)\n",
    "bm_25.k = k\n",
    "\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[faiss_retriever, bm_25], weights=[0.75, 0.25] # you can fine-tune the weights here\n",
    ")\n",
    "\n",
    "correct = 0\n",
    "average_rank = 0\n",
    "num_examples = 400 # Number of examples to evaluate\n",
    "for i, eval_data in enumerate(retriever_evaluation_data[:num_examples]):\n",
    "    returned_docs = ensemble_retriever.invoke(eval_data[\"question\"])\n",
    "    returned_doc_ids = [doc.metadata[\"unique_id\"] for doc in returned_docs]\n",
    "    if eval_data[\"ref_doc_id\"] in returned_doc_ids:\n",
    "        correct += 1\n",
    "        average_rank += 1 / (returned_doc_ids.index(eval_data[\"ref_doc_id\"]) + 1)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "hit_rate = correct / num_examples\n",
    "mrr = average_rank / num_examples\n",
    "\n",
    "print(f\"Hit rate with Hybrid Search @k={k}: {hit_rate}\")\n",
    "print(f\"MRR with Hybrid Search @k={k}: {mrr}\")\n",
    "\n",
    "if mlflow_available:\n",
    "    mlflow.set_experiment(\"Retriever Evaluation\")\n",
    "    with mlflow.start_run(run_name=\"hybrid_retriever\"):\n",
    "        mlflow.log_param(\"k\", k)\n",
    "        mlflow.log_param(\"retriever\", \"hybrid\")\n",
    "        mlflow.log_param(\"weights\", ensemble_retriever.weights)\n",
    "        mlflow.log_metric(\"hit_rate\", hit_rate)\n",
    "        mlflow.log_metric(\"mrr\", mrr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an improvement in the hit rate and MRR after ensembling with BM25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Retrieval Augmented Generation (RAG) pipeline\n",
    "Now that we are satisfied that the retriever is performing reasonably well, we can move on to the generation step. We'll build a basic Chain that given a question will retrieve the relevant context and invoke a Large Language Model to generate the answer. We will use the smaller `mistral.mistral-7b-instruct-v0:2` to generate the responses, this will also be the model that we will fine-tune in the subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.chat_models import ChatBedrockConverse\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm_modelId = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model_id=llm_modelId, max_tokens=1000, temperature=0,\n",
    "    client=bedrock_runtime,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the prompt template that will be used to generate the answer. It's a simple template that will provide basic single-turn functionality and not include any guardrails to constrain the interaction. This is a good starting point but in production scenarios, you would want to add more sophisticated guardrails to ensure the model generates safe and accurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "template = \"\"\"You are a Banking Regulatory Compliance expert. You have been asked to provide guidance on the following question using the referenced regulations below.\n",
    "If the referenced regulations do not provide an answer, indicate to the user that you are unable to provide an answer and suggest they consult with a legal expert.\n",
    "\n",
    "----------------------\n",
    "{context}\n",
    "----------------------\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": ensemble_retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "# produce an output that contains the answer and the context that was passed to the model\n",
    "generate_answer = {\"answer\": prompt | llm | output_parser,\n",
    "                   \"context\": itemgetter(\"context\")}\n",
    "\n",
    "chain = setup_and_retrieval | generate_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invoke the chain with a sample test question and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_record = json.loads(test_data[10])\n",
    "sample_question = sample_record[\"question\"]\n",
    "sample_answer = sample_record[\"answer\"]\n",
    "rprint(f\"[bold green]Sample question:[/bold green] {sample_question}\")\n",
    "response = chain.invoke(sample_question)\n",
    "generated_answer = response[\"answer\"]\n",
    "rprint(f\"[bold green]Generated answer:[/bold green] {generated_answer}\")\n",
    "rprint(f\"[bold green]Ground truth answer:[/bold green] {sample_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Evaluation\n",
    "While a manual examination of the generated answers is one of the most reliable ways to evaluate the model, it may not always be the most scalable especially as we iterate on the pipeline. There are various frameworks for evaluating RAG pipelines. For simplicity, we will look at two metrics that are readily available with [Bedrock Guardrails](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html). Specifically, Bedrock guardrails provide two [Contextual Grounding](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-contextual-grounding-check.html)\n",
    "- **Grounding** – This checks if the model response is factually accurate based on the source and is grounded in the source. Any new information introduced in the response will be considered un-grounded.\n",
    "- **Relevance** – This checks if the model response is relevant to the user query.\n",
    "For example a response that is factually accurate but not relevant to the user query will be considered irrelevant. Similarly a response that is relevant but not factually accurate based on teh provided context will be considered ungrounded.\n",
    "\n",
    "We'll will first compute the average scores for the ground truth answers and then compare them with the scores generated from RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the guardrail should be created as part of the workshop\n",
    "# if not you can create \"rag_eval\" guardrail in the console with only the contextual grounding check enabled\n",
    "eval_guardrail = [gr for gr in bedrock_client.list_guardrails()[\"guardrails\"] if gr[\"name\"]==\"rag_eval\"]\n",
    "if len(eval_guardrail) == 0:\n",
    "    rprint(\"No RAG evaluation guardrail found. Please create one in the Bedrock console.\")\n",
    "else:\n",
    "    eval_guardrail = eval_guardrail[0]\n",
    "eval_guardrail_id = eval_guardrail[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_rag_eval_guardrail(guardrail_id, question, context, response):\n",
    "\n",
    "    guardrail_payload = [\n",
    "        {\n",
    "            \"text\": {\n",
    "                \"text\": context,\n",
    "                \"qualifiers\": [\"grounding_source\"],\n",
    "            }\n",
    "        },\n",
    "        {\"text\": {\"text\": question, \"qualifiers\": [\"query\"]}},\n",
    "        {\"text\": {\"text\": response}},\n",
    "    ]\n",
    "\n",
    "    response = bedrock_runtime.apply_guardrail(\n",
    "        guardrailIdentifier=guardrail_id,\n",
    "        guardrailVersion=\"1\",\n",
    "        source=\"OUTPUT\",\n",
    "        content=guardrail_payload,\n",
    "    )\n",
    "    assessments = response[\"assessments\"][0][\"contextualGroundingPolicy\"][\"filters\"]\n",
    "    grounding_score = [\n",
    "        metric for metric in assessments if metric[\"type\"] == \"GROUNDING\"\n",
    "    ][0][\"score\"]\n",
    "    relevance_score = [\n",
    "        metric for metric in assessments if metric[\"type\"] == \"RELEVANCE\"\n",
    "    ][0][\"score\"]\n",
    "    return {\"grounding_score\": grounding_score, \"relevance_score\": relevance_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on a sample question\n",
    "invoke_rag_eval_guardrail(eval_guardrail_id, sample_question, sample_record[\"context\"], generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_answer_async(rag_chain, example):\n",
    "    \"\"\"Helper function to generate an answer asynchronously\"\"\"\n",
    "    example = json.loads(example)\n",
    "    response = await rag_chain.ainvoke(example[\"question\"])\n",
    "    contexts = [doc.page_content for doc in response[\"context\"]]\n",
    "    row = {\"question\": example[\"question\"], \"answer\": response[\"answer\"], \"contexts\": contexts, \"ground_truth\": example[\"answer\"]}\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll just use the first 100 examples for evaluation\n",
    "NUM_SAMPLE_LLM_EVALUATION = 100\n",
    "eval_rows = []\n",
    "for example in test_data[:NUM_SAMPLE_LLM_EVALUATION]:\n",
    "    eval_rows.append(generate_answer_async(chain, example))\n",
    "event_loop = asyncio.get_event_loop()\n",
    "eval_data= event_loop.run_until_complete(asyncio.gather(*eval_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_guardrail(guardrail_id, questions, contexts, answers):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for question, context, answer in zip(questions, contexts, answers):\n",
    "            results.append(\n",
    "                executor.submit(\n",
    "                    invoke_rag_eval_guardrail, guardrail_id, question, \"\\n\".join(context), answer\n",
    "                )\n",
    "            )\n",
    "        \n",
    "    eval_rows = [result.result() for result in results]\n",
    "        \n",
    "    grounding_scores = [row[\"grounding_score\"] for row in eval_rows]\n",
    "    relevance_scores = [row[\"relevance_score\"] for row in eval_rows]\n",
    "    grounding_score = sum(grounding_scores) / len(grounding_scores)\n",
    "    relevance_score = sum(relevance_scores) / len(relevance_scores)\n",
    "    return grounding_score, relevance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(\"Evaluating the ground truth answers using the RAG evaluation guardrail\")\n",
    "ground_truth_grounding_score, ground_truth_relevance_score = evaluate_rag_guardrail(\n",
    "    eval_guardrail_id,\n",
    "    [row[\"question\"] for row in eval_data],\n",
    "    [row[\"contexts\"] for row in eval_data],\n",
    "    [row[\"ground_truth\"] for row in eval_data],\n",
    ")\n",
    "\n",
    "\n",
    "rprint(\"Evaluating the baseline RAG responses using the RAG evaluation guardrail\")\n",
    "baseline_grounding_score, baseline_relevance_score = evaluate_rag_guardrail(\n",
    "    eval_guardrail_id,\n",
    "    [row[\"question\"] for row in eval_data],\n",
    "    [row[\"contexts\"] for row in eval_data],\n",
    "    [row[\"answer\"] for row in eval_data],\n",
    ")\n",
    "\n",
    "rprint(f\"Ground truth grounding score: {ground_truth_grounding_score}\\nBaseline grounding score: {baseline_grounding_score}\\n\")\n",
    "rprint(f\"Ground truth relevance score: {ground_truth_relevance_score}\\nBaseline relevance score: {baseline_relevance_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results to a file\n",
    "with open(\"base_evaluation.json\", \"w\") as f:\n",
    "    metrics = {\n",
    "        \"ground_truth\": {\n",
    "            \"grounding_score\": ground_truth_grounding_score,\n",
    "            \"relevancy\": ground_truth_relevance_score,\n",
    "        },\n",
    "        \n",
    "        \"baseline\": {\n",
    "            \"grounding_score\": baseline_grounding_score,\n",
    "            \"relevancy\": baseline_relevance_score,\n",
    "        }\n",
    "    }\n",
    "    json.dump(metrics, f)\n",
    "    \n",
    "# log the evaluation results to MLFlow if available\n",
    "if mlflow_available:\n",
    "    mlflow.set_experiment(\"Banking Regulations RAG Evaluation\")\n",
    "    with mlflow.start_run(run_name=\"ground_truth\"):\n",
    "        mlflow.log_metric(\"grounding_score\", ground_truth_grounding_score)\n",
    "        mlflow.log_metric(\"relevance_score\", ground_truth_relevance_score)\n",
    "        \n",
    "    with mlflow.start_run(run_name=\"baseline\"):\n",
    "        mlflow.log_metric(\"grounding_score\", baseline_grounding_score)\n",
    "        mlflow.log_metric(\"relevance_score\", baseline_relevance_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this notebook, we have demonstrated how to use LangChain to build a hybrid search system that combines BM25 and FAISS retrievers to retrieve relevant documents for a given question. We have also shown how to use LangChain to generate answers to questions using a language model and evaluate the generated answers using Bedrock Contextual Guardrails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
