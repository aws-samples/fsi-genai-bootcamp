{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Converse API\n",
    "\n",
    "> *This notebook should work well with the **`conda_python3`** kernel in SageMaker Studio on ml.t3.medium instance*\n",
    "\n",
    "---\n",
    "\n",
    "In the prior notebook, we invoked models using the `invoke_model` API. In this notebook we'll explore the converse API which offers several advantages over the invoke API including:\n",
    "- Standardized inputs and outputs across all models\n",
    "- Better conversation handling via the standardize `message` format\n",
    "- Access to additional functionality such as `tool_usage`\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = \"../..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.environment_validation import validate_environment, validate_model_access\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸš¨ **Caution** You may get an exception running the cell bellow. If that's the case, please restart the kernel by clicking **Kernell** -> **Restart Kernel**. Alternatively click the refresh icon on the notebook toolbar above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_models = [\n",
    "    \"amazon.titan-embed-text-v1\",\n",
    "    \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "    \"us.amazon.nova-pro-v1:0\",\n",
    "]\n",
    "validate_model_access(required_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to interact with AWS APIs\n",
    "import boto3\n",
    "\n",
    "# used for pretty printing\n",
    "from rich import print as rprint\n",
    "from rich.markdown import Markdown, Syntax\n",
    "\n",
    "bedrock = boto3.client(\"bedrock\")                   # administrative API used to interact with Bedrock\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")   # runtime API used to invoke models\n",
    "\n",
    "# See link for available model IDs: https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html\n",
    "MODEL_ID = \"us.amazon.nova-pro-v1:0\" # you can change this to any model you have access to and everything should work without any code changes\n",
    "\n",
    "# See link for available inference configurations: https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InferenceConfiguration.html\n",
    "INFERENCE_CONFIG = {\n",
    "    \"maxTokens\": 1000,\n",
    "    \"temperature\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke model with text prompt\n",
    "\n",
    "We start with the simplest example of invoking a model with a text prompt. We will use the [converse](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) API to interact with the model.\n",
    "The converse API provides a consistent way to interact with different models offered by Bedrock. Each model may have a different set of supported features and modalities which are documented [here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our prompt\n",
    "input_text = \"\"\"What does the ratio that contains a company's cash flow from operations in its numerator and\n",
    "cash outflows from investing and financing activities in its denominator measure?\"\"\"\n",
    "\n",
    "# each input and response are represented as a message JSON object\n",
    "text_prompt_message = {\n",
    "    \"role\": \"user\", # inputs come from the user, responses come from the assistant\n",
    "    \"content\": [\n",
    "        { \"text\": input_text },\n",
    "        ]\n",
    "}\n",
    "\n",
    "# the api expects a list of messages for the entire conversation\n",
    "# you'd add more messages to this list if you were continuing a conversation\n",
    "text_prompt_messages = [text_prompt_message]\n",
    "\n",
    "# invoke the model\n",
    "text_prompt_response = bedrock_runtime.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        inferenceConfig=INFERENCE_CONFIG,\n",
    "        messages=text_prompt_messages\n",
    "    )\n",
    "\n",
    "# extract the text output from the response\n",
    "text_prompt_output = text_prompt_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "rprint(Markdown(text_prompt_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke model with an image prompt\n",
    "\n",
    "Certain models can also be invoked with image data. This can be useful for tasks like image captioning, or extracting data from documents without having to convert them to text through OCR.\n",
    "Let's load and example image and provide it as part of our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO \n",
    "from pathlib import Path\n",
    "image_path = \"data/yield_report_page_3.png\"\n",
    "image = Image.open(image_path)\n",
    "image.show()\n",
    "\n",
    "image_bytes = BytesIO()\n",
    "image.save(image_bytes, format=\"PNG\")\n",
    "image_bytes = image_bytes.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_example_text_prompt = \"\"\"Provide a table and an analysis of the yield trends of AAA rated bonds from 2020 to 2025\"\"\"\n",
    "\n",
    "image_prompt_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"text\": image_example_text_prompt},  # text component of the message\n",
    "        {\n",
    "            \"image\": {\n",
    "                \"format\": \"png\",\n",
    "                \"source\": {\"bytes\": image_bytes},\n",
    "            },  # image component of the message. Images can be passed as bytes, URLs, or base64 strings\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "image_prompt_messages = [image_prompt_message]\n",
    "\n",
    "\n",
    "image_prompt_response = bedrock_runtime.converse(\n",
    "    modelId=MODEL_ID, inferenceConfig=INFERENCE_CONFIG, messages=image_prompt_messages\n",
    ")\n",
    "\n",
    "image_prompt_output = image_prompt_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "rprint(Markdown(image_prompt_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke model with PDF prompt\n",
    "Models can also work directly with documents of various formats such as PDF, MD, DOC, DOCX, XLSX. See [here](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-chatdoc.html) for more details on supported document formats.\n",
    "\n",
    "**Note**: This functionality is mostly useful for enabling document chat where a user can upload a document and ask questions about it. It is not recommended for more complex document processing use-cases where additional pre-processing is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_file_path = (\n",
    "    \"data/Amazon-com-Inc-2023-Shareholder-Letter.pdf\"\n",
    ")\n",
    "\n",
    "doc_bytes = Path(doc_file_path).read_bytes()\n",
    "\n",
    "doc_example_text_prompt = \"\"\"What were some key Amazon product launches in 2023?\"\"\"\n",
    "\n",
    "doc_prompt_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"text\": doc_example_text_prompt},\n",
    "        {\n",
    "            \"document\": {\n",
    "                \"name\": \"Amazon 2023 Shareholder Letter\",\n",
    "                \"format\": \"pdf\",\n",
    "                \"source\": {\"bytes\": doc_bytes},\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "doc_prompt_messages = [doc_prompt_message]\n",
    "\n",
    "\n",
    "doc_prompt_response = bedrock_runtime.converse(\n",
    "    modelId=MODEL_ID, inferenceConfig=INFERENCE_CONFIG, messages=doc_prompt_messages\n",
    ")\n",
    "\n",
    "doc_prompt_output = doc_prompt_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "rprint(Markdown(doc_prompt_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke Model with Tool Use\n",
    "Tool usage is a powerful feature that enables developers to augment the model's capabilities with customized external tools. Underpinning this capability is the model's ability to identify which tool to use for a given task, and generate a request to the tool in a format that is consistent with the tool's signature.\n",
    "\n",
    "In the example below we will create a simple yet powerful tool that can be used to run a python snippet generated by the model and return the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸš¨ **Caution** The example below will enable the model to execute arbitrary python code. This is provided to demonstrate the model's capability to invoke external tools and generate python code and should not be used in production. > Recommended practices for running LLM generated code in production include:\n",
    "> - Running the code in an isolated sandboxed environment\n",
    "> - Limiting the execution time and resources available to the code\n",
    "> - Validating the code before execution\n",
    "> - Restricting the code to a specific set of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import NamedTemporaryFile\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def run_python_code(code: str) -> str:\n",
    "    \"will run the python code and return the output\"\n",
    "    \n",
    "    with NamedTemporaryFile(mode=\"w\", delete=False) as f:\n",
    "        f.write(code)\n",
    "        f.close()\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run([sys.executable, f.name], capture_output=True, text=True, check=True)\n",
    "            return result.stdout\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            return e.stderr\n",
    "        finally:\n",
    "            os.remove(f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the model aware of the available tools, we need to provide the tool's metadata to the model. This metadata includes the tool's name, description, and the input and output formats. The model uses this metadata to generate the request to the tool in a format that is consistent with the tool's signature. Refer to the documentation [here](https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use-inference-call.html) for more details on how to use custom tools use with Bedrock models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_config = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"run_python_code\",\n",
    "                \"description\": f\"Runs the provided python code and returns the standard output\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"code\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"A string representing valid python code to be executed\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"code\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a prompt that should trigger a tool\n",
    "tool_prompt = (\n",
    "    \"Answer the question below. Use tools to help answer any question that requires code or more complex calculations.\\n\"\n",
    "    \"If using code, make sure the code prints any required responses to the standard output.\\n\"\n",
    "    \"QUESTION: A 180-day money market instrument is quoted at an add-on rate of 4.76% for a 360-day year. What is the bond equivalent yield of the instrument? Answer as a percentage with two decimal places.\"\n",
    ")\n",
    "\n",
    "tool_prompt_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"text\": tool_prompt},\n",
    "    ],\n",
    "}\n",
    "\n",
    "tool_prompt_messages = [tool_prompt_message]\n",
    "\n",
    "\n",
    "tool_prompt_response = bedrock_runtime.converse(\n",
    "    modelId=MODEL_ID,\n",
    "    inferenceConfig=INFERENCE_CONFIG,\n",
    "    messages=tool_prompt_messages,\n",
    "    toolConfig=tool_config,  # add the tool configuration to the request\n",
    ")\n",
    "\n",
    "# check if the response contains a tool use \n",
    "if tool_prompt_response[\"stopReason\"] == \"tool_use\":\n",
    "    message_content = tool_prompt_response[\"output\"][\"message\"][\"content\"]\n",
    "    text_content, tool_content = message_content\n",
    "    generated_code = tool_content[\"toolUse\"][\"input\"][\"code\"]\n",
    "    rprint(Markdown(f\"## LLM Reasoning\\n{text_content['text']}\"))\n",
    "\n",
    "    rprint(\n",
    "        Markdown(\"## Generated Code\"),\n",
    "        Syntax(generated_code, \"python\", line_numbers=True, theme=\"lightbulb\"),\n",
    "    )\n",
    "    tool_output = run_python_code(generated_code)\n",
    "    rprint(Markdown(\"## Tool Output\"), Syntax(tool_output, \"python\", theme=\"lightbulb\"))\n",
    "else:\n",
    "    rprint(Markdown(\"## LLM Reasoning\\nNo tool use detected.\"))\n",
    "    rprint(Markdown(\"## LLM Response\\n\"), tool_prompt_response[\"output\"][\"message\"][\"content\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
