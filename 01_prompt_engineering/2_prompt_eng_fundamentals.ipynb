{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prompt Engineering Fundamentals \n",
    "\n",
    "---\n",
    "In this demo notebook, we are going to explore some of the Prompt Engineering Techniques for better model output generation.\n",
    "\n",
    "We demonstrate how to use the boto3 Python SDK to work with *Amazon Bedrock Foundation Models*  We will be using the below models in this notebook;\n",
    "\n",
    "* Amazon Nova Lite v1.0\n",
    "* Amazon Nova Micro v1.0\n",
    "* Anthropic Claude Haiku v3.5\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dadd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install necessary libraries\n",
    "import sys\n",
    "import os\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.environment_validation import validate_environment, validate_model_access\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044f158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the below show that you do not have these model accesses, please go to the Bedrock console to get access.\n",
    "required_models = [\n",
    "    \"amazon.nova-lite-v1:0\",\n",
    "    \"amazon.nova-micro-v1:0\",\n",
    "    \"anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "]\n",
    "validate_model_access(required_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b2a05-78a9-40ca-9b5e-121030f9ede1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rich for pretty printing\n",
    "from rich import print as rprint\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "# boto3 for AWS SDK\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c272d256-95a5-48ef-92d3-3bf012487b82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# as with other AWS services Bedrock exists in Control Plane (CP) and Data Plane (Runtime) actions.\n",
    "bedrock_cp = boto3.client(\"bedrock\")\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9174c4-326a-463e-92e1-8c7e47111269",
   "metadata": {},
   "source": [
    "#### Validate the connection\n",
    "\n",
    "We can check the client works by trying out the `list_foundation_models()` method, which will tell us all the models available for us to use.  These are models that are available, and not necessarily the models that you have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b4466-12ff-4975-9811-7a19c6206604",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "foundation_models = bedrock_cp.list_foundation_models()\n",
    "rprint(foundation_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f690043-df45-448f-8fa6-1ea8b06f1087",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## The Bedrock `Converse` API\n",
    "\n",
    "Bedrock's [`Converse` API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) simplifies the calling of LLMs, providing a uniform interface across the different models. The `Converse` API offers;\n",
    "\n",
    "* Standardized inputs and outputs across all models\n",
    "* Better conversation handling via the standardize message format\n",
    "* Access to additional functionality such as tool_usage\n",
    "\n",
    "We will be using the below `Converse` API call in this notebook.\n",
    "\n",
    "```\n",
    "   response = bedrock_runtime.converse(\n",
    "        modelId=model_id, # or inference_profile id\n",
    "        inferenceConfig=inference_configs,\n",
    "        system=system_prompts,\n",
    "        messages=messages\n",
    "    )\n",
    "```\n",
    "\n",
    "Let's build each of the above parameters below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670c3afe-0505-4715-8228-6866ae718010",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### `modelId` and Inference Profiles\n",
    "This field indicates the model to be used.   E.g., for Amazon Nova Micro 1.0, it'd be `amazon.nova-micro-v1:0`\n",
    "\n",
    "The `modelId` field can take either a model id or an inference profile id. [Inference profile](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles.html), allows for automatic routing of model invocations across multiple regions. With this capability, you can burst capacity across the different regions, seamlessly.  You can [create your own inference profiles](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles-create.html) to track model usage across multiple users, projects, lines of business, etc.\n",
    "\n",
    "We will be using Amazon managed inference profiles in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003393a0-43e9-44f9-b423-a05a2f86ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_profiles = bedrock_cp.list_inference_profiles()\n",
    "rprint(inf_profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03fcfcf-eca2-441d-98d9-aab57e88cca8",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### Inference Profiles\n",
    "Let's store the Amazon managed inference profiles for each of our models in individual variables.  We will use these variables in the `modelId` field of the `Converse` API call.\n",
    "\n",
    "```python\n",
    "modelId = inf_profile_nova_micro_v1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab64aad-98c1-4ab6-80ec-3c3ae8d8946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inf_prof in inf_profiles['inferenceProfileSummaries']:\n",
    "    if inf_prof['inferenceProfileName'] == 'US Nova Micro':\n",
    "        rprint(inf_prof)\n",
    "        inf_profile_nova_micro_v1 = inf_prof['inferenceProfileId']\n",
    "\n",
    "    if inf_prof['inferenceProfileName'] == 'US Nova Lite':\n",
    "        rprint(inf_prof)\n",
    "        inf_profile_nova_lite_v1 = inf_prof['inferenceProfileId']\n",
    "\n",
    "    if inf_prof['inferenceProfileName'] == 'US Anthropic Claude 3.5 Haiku':\n",
    "        rprint(inf_prof)\n",
    "        inf_profile_claude_haiku_v3_5 = inf_prof['inferenceProfileId']\n",
    "        \n",
    "\n",
    "rprint(\"This the lightest Amazon Nova LLM.  We will be using this model in most examples of this notebook.\\n\", inf_profile_nova_micro_v1, \"\\n\")\n",
    "rprint(\"This is the smallest model in the Anthropic family of models\", inf_profile_claude_haiku_v3_5, \"\\n\")\n",
    "rprint(\"This is the midrange Amazon Nova LLM.  We will use it in the multi-modal use case.\\n\", inf_profile_nova_lite_v1, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3582600-4eee-450b-95ec-8dfcc7868cac",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### `inferenceConfig`\n",
    "Inference parameters allow you to control the behavior of the model.  The `inferenceConfig` field is a dictionary that can contain the following parameters:\n",
    "\n",
    "temperature (0.0 to 1.0, default 0.7):\n",
    "\n",
    "- Lower values (like 0.0) make responses more deterministic although this is not guaranteed\n",
    "- With a value of 0.0, the model will choose the most likely next token, higher values (like 1.0) will make the model choose other less likely tokens\n",
    "- Higher values (like 1.0) make responses more randomized\n",
    "\n",
    "maxTokens (integer, default 5k):\n",
    "- Controls the maximum length of the model's response\n",
    "- Model's response may get truncated if it exceeds this value\n",
    "\n",
    "topP (0.0 to 1.0, default 0.9):\n",
    "- Each token is sampled based on its probability of being the next token given the previous tokens\n",
    "- The model will only consider the top P% cumulative probability mass for the next token\n",
    "   \n",
    "Note that not all models support all inference parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64728375-e559-4d83-8373-ea83fdfd75ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_configs = {\n",
    "    \"temperature\": 0.5,        # Controls randomness (0.0 to 1.0)\n",
    "    \"maxTokens\": 2048,         # Maximum tokens in the response\n",
    "    \"topP\": 0.9,               # Sample from the top 90% of the probability mass\n",
    "\n",
    "}\n",
    "rprint(inference_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f257c6a-23d2-44c7-9c5f-45dd62178627",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### `system` Prompts\n",
    "\n",
    "System prompts are your (the developer's) directive to the LLM, as opposed to the user's.  It is useful for setting the pretext of the LLM task and defining the role of the LLM.  For example, you can set the system prompt to be a \"helpful assistant\" or \"a sarcastic assistant\".  The system prompt is not visible to the end user.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda3359-8d13-4090-94f3-c2dc13b883ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompts = [{\"text\": \"You are a financial advisor of a large asset management firm.\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa14598-7ff3-43c4-91dd-fed6ddc57b3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "#### `messages`\n",
    "\n",
    "Messages are the conversation history between the user and the LLM.  The messages are a list of dictionaries, where each dictionary contains the following fields:\n",
    "- `role`: The role of the message sender.  This can be either `user` or `assistant`\n",
    "- `content`: The content of the message.  This can include text, images, files, etc.\n",
    "\n",
    "Every component of the interaction is recorded as a list of messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb9b94-101f-46c5-b5b2-59135212bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [] # start with an empty convesation\n",
    "user_msg1 = \"What is the best way to save for my children's college tuitions?\"\n",
    "\n",
    "message_1 = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": user_msg1}]\n",
    "}\n",
    "\n",
    "messages.append(message_1)  # later append outputs for a conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce22c308-ebbf-4ef5-a823-832b7c236e31",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Let's now call Bedrock!\n",
    "\n",
    "Let's first try with Amazon Nova Micro v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b80e254-1118-4248-93e5-893635ada3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_runtime.converse(\n",
    "    modelId=inf_profile_nova_lite_v1,\n",
    "    messages=messages,\n",
    "    system=system_prompts,\n",
    "    inferenceConfig=inference_configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0a7b99-9d60-4a7a-85db-9fae09f7d2f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rprint(response)\n",
    "rprint(response['output']['message']['content'][0]['text'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fff5e7d-00ef-44c3-bc93-328b31dcf18a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Bedrock Standardizes Different Model Inputs\n",
    "As previously mentioned, Bedrock Converse API allows the developer to invoke different models with a standard interface.  This means that the same API call can be used to invoke different models, even if they come from different model providers.\n",
    "\n",
    "Let's now try the same call that we did with Amazon Nova Micro v1, but with Anthropic Claude Haiku v3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f3c9ef-54ed-4ece-a7d0-7ef560c199ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_runtime.converse(\n",
    "    modelId=inf_profile_claude_haiku_v3_5,\n",
    "    messages=messages,\n",
    "    system=system_prompts,\n",
    "    inferenceConfig=inference_configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80f43a-dfc1-4c93-a94d-311b6bef8753",
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(response)\n",
    "rprint(response['output']['message']['content'][0]['text'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48a0e8-147d-4525-a6b2-68a09af1b2c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prompt Engineering - Basic Techiques\n",
    "\n",
    "In this section, we are going to explore some fundamental prompt engineering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c0ae11-c2cb-4c0c-b656-582703d1673b",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### Zero-shot prompting\n",
    "Zero Shot prompting describes the technique where we present a task to an LLM without giving it further examples. We therefore, expect it to perform the task without getting a prior “shot” at the task. Hence, “zero-shot” prompting. Modern LLMs demonstrate remarkable zero-shot performance and a positive correlation can be drawn between model size and zero-shot performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7079ffff-9356-44e6-9f4d-ee16d2f08964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "zero_shot_system_prompt = \"\"\"You are a customer service agent tasked with classifying emails by type. Please output your answer and then justify your classification. How would you categorize this email? \"\"\"\n",
    "\n",
    "zero_shot_prompt = \"\"\"\n",
    "<email>\n",
    "I would like to know how I can contribute to my retirement account? Any additional resources to read about IRA would be helpful.\n",
    "</email> \n",
    "Provide and explanation for your choice of answer\n",
    "\n",
    "The categories are: \n",
    "(A) IRA \n",
    "(B) 529 Plan\n",
    "(C) Cash Management\n",
    "(D) Youth Account\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8e46c7-a7cd-4fab-804e-18ed09f309be",
   "metadata": {
    "tags": []
   },
   "source": [
    "The xml tags above were historically used in earlier Anthropic Claude models to breakdown the prompt into logical segments. Models from other providers, and the current Anthropic Claude models 3+, do not require these xml tags. But they can still be useful in prompt construction as they allow you to reference specific parts of the prompt.  For example, you can use the `<input>` tag to reference the input text in the prompt.  This is useful for creating dynamic prompts that can change based on the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36615c11-3599-4ba3-bd78-dc8a8e1c81b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try this with Amazon Nova Lite v1\n",
    "\n",
    "zero_shot_msg = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": zero_shot_prompt}]\n",
    "}\n",
    "\n",
    "zero_shot_messages = [zero_shot_msg]\n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=inf_profile_nova_lite_v1,\n",
    "    messages=zero_shot_messages,\n",
    "    system=[{\"text\": zero_shot_system_prompt}],\n",
    "    inferenceConfig=inference_configs\n",
    ")\n",
    "\n",
    "rprint(response['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fbaa71-dc6c-420d-9f94-3376c8493769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try the same prompt with Anthropic Claude Haiku 3.5.  Again, there is no change to the converse API call.  Only the modelId value changes.\n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=inf_profile_claude_haiku_v3_5, # note the modelId change\n",
    "    messages=zero_shot_messages,\n",
    "    system=[{\"text\": zero_shot_system_prompt}],\n",
    "    inferenceConfig=inference_configs\n",
    ")\n",
    "\n",
    "rprint(response['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe215b9-1482-4e0e-a3d4-56880393a3bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### Few-shot prompting\n",
    "Giving the model more information about the tasks at hand via examples is called Few-Shot Prompting. It can be used for in-context learning by providing examples of the task and the desired output. We can therefore condition the model on the examples to follow the task guidance more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db72b95a-5855-4699-a499-23cc6bb9ea10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "few_shot_prompt = \"\"\"Your job is to generate financial summaries similar to the examples below: \n",
    "\n",
    "Microsoft: Microsoft reported record revenue of $51.9 billion in its latest quarterly earnings report, a 20% increase year-over-year. Its net income rose to $16.7 billion, up 21% compared to the previous year. Microsoft's revenue growth was driven by its cloud computing business Azure and Office 365 productivity tools.\n",
    "\n",
    "Amazon: In its most recent quarterly results, Amazon reported net sales of $96.1 billion, an increase of 15% compared to the same period last year. Amazon's net income decreased to $2.9 billion, down 58% year-over-year, as the company faced rising costs due to inflation and supply chain issues. Amazon Web Services, its cloud computing segment, continued to see strong growth with sales up 33%.\n",
    "\n",
    "Now provide a brief financial summary for Apple Inc. based on the following key facts:\n",
    "- Revenue for the last 12 months: $260 billion \n",
    "- Net income: $55 billion\n",
    "- Total assets: $321 billion\n",
    "- Market capitalization: $2.2 trillion\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202bee5a-bcf4-4b06-b2ab-68c29b82b2b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will be using Amazon Nova Micro v1, unless stated otherwise.  But please feel free to try with Haiku or the other models.  \n",
    "# If you wish to try models other than Amazon Nova Micro v1, Amazon Nova Lite v1 or Anthropic Claude Haiku 3.5, please ensure that you enable them first in the Bedrock console.\n",
    "\n",
    "\n",
    "few_shot_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": few_shot_prompt}]\n",
    "}\n",
    "\n",
    "few_shot_messages = [few_shot_message]\n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=inf_profile_nova_lite_v1,\n",
    "    messages=few_shot_messages,\n",
    "    inferenceConfig=inference_configs\n",
    ")\n",
    "\n",
    "rprint(response['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b831f3a-0583-4b97-acfa-1379acd74213",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### Chain of Thought prompting\n",
    "Chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding. The main idea of CoT is that by showing the LLM some few shot exemplars where the reasoning process is explained in the exemplars, the LLM will also show the reasoning process when answering the prompt. This explanation of reasoning often leads to more accurate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba3bec-9053-46a7-a526-83919153b16f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "cot_prompt = \"\"\"\n",
    "Apple Inc. key facts:\n",
    "- Revenue for the last 12 months: $260 billion \n",
    "- Operating Expenses: $205 billion\n",
    "- Total assets: $321 billion\n",
    "- Market capitalization: $2.2 trillion\n",
    "\n",
    "If Apple's revenue is expected to grow by 10% in the next year, however it's expenses are expected to increase by 15%, what would be the expected net income for Apple in the next year?\n",
    "\n",
    "Think step-by-step and provide your thought process in the <thinking></thinking> XML tags and the final answer in the <answer></answer> XML tags.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3d1929-8950-4937-a005-546bb8b2b715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "cot_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": cot_prompt}]\n",
    "}\n",
    "\n",
    "cot_messages = [cot_message] \n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=inf_profile_nova_lite_v1,\n",
    "    messages=cot_messages,\n",
    "    inferenceConfig=inference_configs\n",
    ")\n",
    "\n",
    "rprint(response['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf24ea7-7195-4286-8782-85d776c3673e",
   "metadata": {},
   "source": [
    "## Prompt Engineering - Different task examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc4596-d76e-4abe-a77e-0259db5ed14e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Conversation Summarization\n",
    "Prompt engineering for conversation summarization is a process of crafting prompts that guide a language model like Nova in summarizing dialogues or conversations effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5611d319-f3a6-4705-a84c-11a666b41324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarization_prompt = \"\"\"\n",
    "Provide a concise bullet-point summary of the following conversation\n",
    "<conversation>\n",
    "Susan: Welcome back, Chuck. This is the second part of our interview, and I want to ask you these questions as quickly as possible because I know you have a flight to catch, so. . . .\n",
    "\n",
    "Chuck: No problem. I’m happy to chat with you.\n",
    "\n",
    "Susan: It says on your website that you never studied business before accepting a marketing role at a Spanish-speaking multimedia publisher. Was … was that—\n",
    "\n",
    "Chuck: Yeah.\n",
    "\n",
    "Susan: So was that more of a stretch, or … ?\n",
    "\n",
    "Chuck: Yeah, well, actua—\n",
    "\n",
    "Susan: Oh, I’m sorry, just a minute. Can everyone else just put themselves on mute, please?\n",
    "\n",
    "Akim: [inaudible 00:37]\n",
    "\n",
    "Susan: Great, thanks. OK, so where were we? So, I was asking you if, when you took the new role as Director, it felt like a stretch?\n",
    "\n",
    "Chuck: Right, yeah. . . . I knew there would be a period of adjustment that I just would have to push through, you know?\n",
    "\n",
    "Susan: Absolutely. Were there any other challenges or roadblocks that you weren’t expecting though?\n",
    "\n",
    "Chuck: Hm, roadblocks I wasn’t expecting. Um…\n",
    "\n",
    "Susan: [laughs] I keep throwing you curveballs.\n",
    "\n",
    "Chuck: No, it’s OK. So, I actually didn’t anticipate the workload involved in learning a foreign language on the job. It’s like, you know, sometimes. . . . I mean, you need to be really flexible and ready to change it up if your strategy isn’t actually working. Know what I mean?\n",
    "\n",
    "Susan: [laughs] Definitely. So what was your budget like, I mean, was that, like, a challenge, too?\n",
    "\n",
    "Chuck: Hoo boy. [laughs] Yeah it definitely was. We had funding available, but we just, um, needed so many hands on deck. It was hard to, you know, actually manage so many people with our existing resources. [coughs]\n",
    "\n",
    "Susan: I totally understand. So what was your next move? I mean, did you—\n",
    "\n",
    "Chuck: Yeah, yeah. . . . So next, I kind of wanted to see where I do a gap analysis and throw resources at new initiatives where we weren’t, uh, I mean, I wanted to actually close the loop in those places where we didn’t quite have a foothold.\n",
    "\n",
    "Susan: Nice, nice.\n",
    "\n",
    "Chuck: Yeah, we really just needed to keep what was working and then lay out a new plan for [clears throat] future growth.\n",
    "\n",
    "Susan: So, it looks like we have time for maybe, um, let’s see— maybe one more question.\n",
    "\n",
    "Chuck: Sounds good. Shoot.\n",
    "\n",
    "Susan: How did this experience change the way you approached new challenges going forward?\n",
    "\n",
    "Chuck: Hm, good one. [laughs] I think that I was less, uh, likely to question my own judgement when it came to adaptability. Now I know that I really have it in me, you know?\n",
    "\n",
    "Really all it takes is a great marketing team underneath you and the freelance resources to put together the best marketing initiative you can afford, you know, with, um, the standard tools high-quality content, good ad placement, good SEO, a strong social presence and then maybe debut a new idea every six months. Um, like, you could come up with a certain theme or catch phrase and actually weave it through all your branding and regular initiatives.\n",
    "\n",
    "Susan: OK, well thanks, Chuck. This is really good stuff. I know you have to run, um, but I’d just like to thank you for, you know, taking this time with me today.\n",
    "\n",
    "Chuck: Sure, sure. Of course\n",
    "</conversation>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92dd02c-3900-489f-95f7-3c7ac1bae9ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarization_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": summarization_prompt}]\n",
    "}\n",
    "\n",
    "summarization_messages = [summarization_message]\n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=inf_profile_nova_lite_v1,\n",
    "    messages=summarization_messages,\n",
    "    inferenceConfig=inference_configs\n",
    ")\n",
    "\n",
    "rprint(response['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c65925-1a32-4d05-971e-74e2778d4b0c",
   "metadata": {},
   "source": [
    "### Question & Answering\n",
    "Prompt engineering for question answering is a process of crafting prompts that guide a language model like Nova in answering questions on provided context effectively. This is used as a basis for more advanced techniques such as Retrieval-Augmented Generation (RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c60cfc-118b-4025-afd7-9585acbb2221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are a customer service agent tasked with answering questions about various financial products and services. Please output your answer and then justify your classification. \"\"\"\n",
    "\n",
    "qa_prompt = \"\"\"\n",
    "<context>\n",
    "When the teen turns 18, their account is eligible to transition to the regular Fidelity brokerage account with expanded features like option and margin trading.\n",
    "The teen will be prompted to transition their account starting on their 18th birthday. They will have 60 days to do so before their debit card and ability to trade will be restricted. Once the account is transitioned, the debit card that the teen was issued for their Fidelity Youth™ Account will continue to be valid until it expires; at that point, a new debit card will be issued.\n",
    "Once a teen turns 18, the teen may choose whether or not the parent/guardian will continue to have access to that teen’s Fidelity Youth Account information. Teens can still use the Fidelity Youth™ app when they turn 18, however additional capabilities are available to them in the Fidelity mobile app.\n",
    "</context> \n",
    "\n",
    "<question> What is the role of a parent/ guardian according to Fidelity youth accounts? </question>\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b112947e-dd93-416d-8578-67cc4cc06656",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": qa_prompt}]\n",
    "}\n",
    "\n",
    "qa_messages = [qa_message]\n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=inf_profile_nova_lite_v1,\n",
    "    messages=messages,\n",
    "    system=[{\"text\": qa_system_prompt}],\n",
    "    inferenceConfig=inference_configs\n",
    ")\n",
    "\n",
    "rprint(response['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4c75d1-ea68-4db0-b1fe-3f195c5206f9",
   "metadata": {},
   "source": [
    "### Content Generation\n",
    "We can use prompt engineering to guide the model in generating content that is relevant to the task at hand. This can be used for generating content for a variety of tasks such as content creation, content summarization, and more.\n",
    "In this example, the model will generate a table using provided data and as well as some questions that it can answer based on this data. This approach is common for getting the model to address more complex research type tasks where multiple steps are involved in generating the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c2ebf-b9a8-473b-8c1a-16daec656857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content_generation_prompt = \"\"\"Below is research that you performed previously. On basis of this research please answer the question below. \n",
    "If it does not include the exact answer, please respond with relevant information from the research in the response. Prefer more recent sources when possible.\n",
    "If the context contains publication ids, please include citations in the response. Citations should be formatted as [publication_id] or [org_id]\n",
    "If the user asks for a chart, return only machine readable json structure for the chart in HighCharts format without any formatting. \n",
    "Prefer answering in First Person (I, me, my) style.\n",
    "In your response, after you answer draw a line, and include 3 additional questions you can answer with the research, one question per line.\n",
    "Example:\n",
    "{Your Answer}\n",
    "\n",
    "Additional questions I can answer::\n",
    "Question 1\n",
    "Question 2\n",
    "Question 3\n",
    "\n",
    "<research>\n",
    "JPMorgan Chase Bank, N.A. (164500) Financials: \n",
    " \n",
    "|Indicator|2020(annual)|2021(annual)|2022(annual)|\n",
    "|---|---|---|---|\n",
    "|Total assets (USD Billion)|3025.28|3306.98|3201.94|\n",
    "|Tangible Common Equity (USD Billion)|221.41|262.33|280.96|\n",
    "|Problem Loans / Gross Loans|1.64|1.14|1.00|\n",
    "|Tangible Common Equity / Risk Weighted Assets|16.48|18.83|19.04|\n",
    "|Problem Loans / (Tangible Common Equity + Loan Loss Reserve)|6.71|4.44|3.80|\n",
    "|Net Interest Margin|2.13|1.74|2.22|\n",
    "|PPI / Average RWA|3.30|2.90|3.45|\n",
    "|Net Income / Tangible Assets|0.70|1.16|1.09|\n",
    "|Cost / Income Ratio|58.76|61.90|57.71|\n",
    "|Market Funds / Tangible Banking Assets|15.38|12.38|12.43|\n",
    "|Liquid Banking Assets / Tangible Banking Assets|55.38|55.31|49.91|\n",
    "|Gross Loans / Due to Customers|47.24|44.01|48.72|\n",
    " \n",
    "Bank of America Corporation (541000) Financials: \n",
    " \n",
    "|Indicator|2020(annual)|2021(annual)|2022(annual)|\n",
    "|---|---|---|---|\n",
    "|Total assets (USD Billion)|2786.01|3146.17|3024.42|\n",
    "|Tangible Common Equity (USD Billion)|174.68|175.58|190.40|\n",
    "|Problem Loans / Gross Loans|1.18|0.95|0.91|\n",
    "|Tangible Common Equity / Risk Weighted Assets|12.74|12.55|13.49|\n",
    "|Problem Loans / (Tangible Common Equity + Loan Loss Reserve)|5.92|5.26|4.87|\n",
    "|Net Interest Margin|1.75|1.52|1.82|\n",
    "|PPI / Average RWA|1.94|1.96|2.23|\n",
    "|Net Income / Tangible Assets|0.67|0.92|0.71|\n",
    "|Cost / Income Ratio|67.23|69.57|66.12|\n",
    "|Market Funds / Tangible Banking Assets|21.09|21.26|22.71|\n",
    "|Liquid Banking Assets / Tangible Banking Assets|57.50|58.98|55.10|\n",
    "|Gross Loans / Due to Customers|54.20|50.35|56.05|\n",
    " \n",
    "Wells Fargo Bank, N.A. (811500) Financials: \n",
    " \n",
    "|Indicator|2020(annual)|2021(annual)|2022(annual)|\n",
    "|---|---|---|---|\n",
    "|Total assets (USD Billion)|1767.81|1779.50|1717.53|\n",
    "|Tangible Common Equity (USD Billion)|145.47|148.21|149.95|\n",
    "|Problem Loans / Gross Loans|1.69|1.36|1.14|\n",
    "|Tangible Common Equity / Risk Weighted Assets|14.36|15.35|15.34|\n",
    "|Problem Loans / (Tangible Common Equity + Loan Loss Reserve)|9.18|7.37|6.45|\n",
    "|Net Interest Margin|2.41|2.13|2.77|\n",
    "|PPI / Average RWA|1.46|1.91|2.22|\n",
    "|Net Income / Tangible Assets|0.20|1.00|0.95|\n",
    "|Cost / Income Ratio|75.75|69.83|67.73|\n",
    "|Market Funds / Tangible Banking Assets|5.58|3.44|6.68|\n",
    "|Liquid Banking Assets / Tangible Banking Assets|41.44|41.29|35.80|\n",
    "|Gross Loans / Due to Customers|61.22|57.78|65.53|\n",
    " \n",
    "</research>\n",
    " \n",
    "<analysis>\n",
    " \n",
    "(Doc Id: PBC_1362054, Publish Date: 2023-05-11, Title:Bank of America Corporation: Update to credit analysis following ratings upgrade)\n",
    " \n",
    "1. Support and Structural Considerations:\n",
    "About Moody's Bank ScorecardOur Bank Scorecard is designed to capture, express and explain in summary form our Rating Committee's judgment. When read in conjunction with our research, a fulsome presentation of our judgment is expressed. As a result, the output of our Scorecard may materially differ from that suggested by raw data alone (though it has been calibrated to avoid the frequent need for strong divergence). The Scorecard output and the individual scores are discussed in rating committees and may be adjusted up or down to reflect conditions specific to each rated entity. As per Moody's Banks rating methodology, the historic ratios in the scorecard for Capital are as of most recent period, for Asset Risk and Profitability they are the worse of the most recent year-to-date period or the average of the last three years and the most recent year-to-date, and for Funding Structure and Liquid Resources they are as of the most recent year-end.\n",
    "2. RATINGS:\n",
    "Please see the ratings section at the end of this report for more information. The ratings and outlook shown reflect information as of the publication date. \n",
    "Summary:\n",
    "Bank of America Corporation (BAC, A1 stable) is the parent holding company for the second largest banking group in the US. The group is a global systemically important bank operating primarily through its principal bank subsidiary, Bank of America N.A. (BANA, Aa1 stable, a2 baseline credit assessment). BAC's credit profile is supported by its conservative risk appetite, a balanced and diversified business mix, strong funding and liquidity, robust cost discipline, strengthened capital and resilient profitability. During 2022, in response to increases in its regulatory capital requirements, BAC boosted its capital ratios significantly.\n",
    " \n",
    "</analysis>\n",
    " \n",
    "<questions>\n",
    "1. Can you compare the financials for JPMC, Bank of America and Wells fargo in a table format?\n",
    "2. For statements referenced from documents include a reference in the format [docId].\n",
    "DO NOT UNDER ANY CIRCUMSTANCES USE ANY DATA OTHER THAN MENTIONED IN THE RESEARCH SECTION.\n",
    "</questions>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0539fcb9-bb51-41a1-9d66-02651358219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_generation_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": content_generation_prompt}]\n",
    "}\n",
    "content_generation_messages = [content_generation_message]\n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=inf_profile_nova_lite_v1,\n",
    "    messages=content_generation_messages,\n",
    "    inferenceConfig=inference_configs\n",
    ")\n",
    "\n",
    "rprint(Markdown(response['output']['message']['content'][0]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450c5d1f",
   "metadata": {},
   "source": [
    "### Multi-modal prompts\n",
    "In addition to text, some models allow for text and vision prompt capability.\n",
    "\n",
    "This is useful in a number of use cases including:\n",
    "- Extracting information from charts, graphs, diagrams, etc\n",
    "- Summarizing rich content\n",
    "- Answering questions based on images\n",
    "- Being able to effective handle more complex format types. For example we can ask the model to narrate a PowerPoint presentation and then utilize the narrative in downstream semantic search applications\n",
    "\n",
    "We will now be using Amazon Nova Lite, which allows for text and vision inputs.  Amazon Nova Micro is for text only.\n",
    "\n",
    "Let's look at an example using a page out of [JPMC's 2022 Annual Report](data/jpmc_annual_report_page_6.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca331ac-a43d-451e-a6a5-a85b6b6bb66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the pdf document\n",
    "# You can also find this document directly under \"fsi-genai-bootcamp/02_prompt_engineering/data/\"\n",
    "# The code in this cell is just used to view the document, it is not used in the Bedrock prompt\n",
    "\n",
    "pdf_path = \"data/jpmc_annual_report_page_6.pdf\"\n",
    "from base64 import b64encode\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def display_pdf(pdf_path, width=1000, height=700):\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        pdf_bytes = f.read()\n",
    "    base64_pdf = b64encode(pdf_bytes).decode('utf-8')\n",
    "    display(HTML(f'<embed src=\"data:application/pdf;base64,{base64_pdf}\" width=\"{width}\" height=\"{height}\" type=\"application/pdf\"/>'))\n",
    "\n",
    "\n",
    "display_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e83d0d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data_text = \"\"\"\n",
    "Extract the data contained in the chart and provide it in a json format. The data should include the Net Income, Diluted EPS, and ROTCE from 2004 to 2022 and should be structured as follows:\n",
    "{\"net_income\":{\"units:\"$B\", \"data_points\":{\"2004\": 4.5}}, \"diluted_eps\":{...}, \"rotce\":{...}}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a0bf29-3859-4a4b-ab09-4ac9d98da82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_as_bytes(pdf_path):\n",
    "    \"\"\"Reads a PDF file and returns its content as bytes.\"\"\"\n",
    "    with open(pdf_path, \"rb\") as pdf_file:\n",
    "        pdf_bytes = pdf_file.read()\n",
    "    return pdf_bytes\n",
    "\n",
    "\n",
    "pdf_path = \"data/jpmc_annual_report_page_6.pdf\"\n",
    "\n",
    "pdf_content = read_pdf_as_bytes(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c9dfcf-80d7-41b3-bfe4-58521025ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note how the message has a text and an document component\n",
    "# Amazon Nova multi-modal models can read text, documents and images\n",
    "# Some multi-modal models have image reading capability only, and documents such as pdf will need to be converted into images beforehand\n",
    "# E.g., the below message input will not work with Anthropic Claude 3.5, unless the pdf is first converted into image format\n",
    "\n",
    "pdf_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"text\": prompt_data_text},\n",
    "        {\n",
    "            \"document\": {\n",
    "                \"format\": \"pdf\",\n",
    "                \"name\": \"jpmc_annual_rpt\",\n",
    "                \"source\": {\"bytes\": pdf_content},\n",
    "            }\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "pdf_messages = [pdf_message]\n",
    "\n",
    "response = bedrock_runtime.converse(\n",
    "    modelId=inf_profile_nova_lite_v1,\n",
    "    messages=pdf_messages,\n",
    "    inferenceConfig=inference_configs,\n",
    ")\n",
    "\n",
    "rprint(response)\n",
    "rprint(Markdown(response['output']['message']['content'][0]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cecb9a4-ebd1-479d-bbf8-af3da3e53b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "226d69cc-7cfb-4066-9a4a-2c846a11e02c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We are just getting warmed up, we explored basic prompting techiques. Please proceed to the next Notebook."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
